{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ce197e",
   "metadata": {},
   "source": [
    "# Tuesday Afternoon: Inference after Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009414ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import numpy_indexed as npi\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f149cf9",
   "metadata": {},
   "source": [
    "# Part 1: Simulating \"Running\" an MRT with Pooling RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000f88a",
   "metadata": {},
   "source": [
    "## Simple Data Generating Environment\n",
    "- \"Covariates\": $O_t = (1, B_t, C_t, D_t)$ where $B_t$ is binary and $C_t$ is continuous and $D_t$ is a measure of \"dosage\"\n",
    "    - Specifically $D_t = \\frac{1}{1-\\gamma} \\sum_{t'=1}^{t-1} \\gamma^{t'} A_{t'}$ for $\\gamma = 0.95$. We normalize by $1-\\gamma$ to ensure $D_t \\in [0,1]$.\n",
    "- Binary action $A_t \\in \\{0, 1\\}$\n",
    "- Rewards are generated as follows:\n",
    "\n",
    "$$\n",
    "R_{t+1} = f_0(O_t)^\\top \\alpha_0 + A_t f_1(O_t)^\\top \\alpha_1 + \\epsilon_t\n",
    "$$\n",
    "\n",
    "where $f_0(O_t) = (1, B_t, C_t, D_t)$, $f_1(O_t) = (1, C_t)$, \n",
    "\n",
    "\n",
    "$\\alpha_0 = (1, -0.8, 0.05, -0.7)$, $\\alpha_1 = (0.3, 0)$, $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_{\\mathrm{env}}^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_params = {\n",
    "    \"alpha0\": np.array([1, 0.5, 0.2, -0.7]),\n",
    "    \"alpha1\": np.array([0.3, 0.05]),\n",
    "    \"sigma_env\": 0.5,\n",
    "    \"B_p\": 0.3,\n",
    "}\n",
    "\n",
    "def generate_reward(state, action, env_params):\n",
    "    base_state = np.array([1, state[1], state[2], state[3]])\n",
    "    treat_state = np.array([1, state[2]])\n",
    "    \n",
    "    base_mean_reward = np.dot( base_state, env_params[\"alpha0\"] )\n",
    "    mean_reward = base_mean_reward + \\\n",
    "                    action * np.dot( treat_state, env_params[\"alpha1\"] )\n",
    "    reward = mean_reward + np.random.normal(scale=env_params[\"sigma_env\"])\n",
    "    return reward\n",
    "        \n",
    "\n",
    "def generate_state(prev_state, prev_action, prev_reward, env_params):\n",
    "    B = np.random.binomial(1, env_params[\"B_p\"])\n",
    "    C = np.random.normal(3, scale=1)\n",
    "    \n",
    "    # Form dosage update\n",
    "    gamma = 0.95\n",
    "    norm_gamma = 1/(1-gamma)\n",
    "    if prev_state is None:\n",
    "        dosage = np.random.binomial(1, 0.5)\n",
    "    else:\n",
    "        dosage = (gamma*norm_gamma*prev_state[-1] + prev_action ) / norm_gamma\n",
    "    \n",
    "    state = np.array([1, B, C, dosage])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248d34b",
   "metadata": {},
   "source": [
    "## Pooling Boltzmann Sampler\n",
    "\n",
    "- Boltzmann or Softmax sampling algorithm\n",
    "- RL algorithm's model of the reward:\n",
    "\n",
    "$$\n",
    "\\mathbb{E} [ R_{i,t+1} | H_{i,t-1}, S_{i,t}, A_{i,t} ]= \\phi_0(S_t)^\\top \\beta_0 + A_{i,t} \\phi_1(S_{i,t})^\\top \\beta_1\n",
    "$$\n",
    "\n",
    "where $\\phi_0(S_{i,t}) = (1, C_{i,t})$, $\\phi_1(S_{i,t}) = (1, C_{i,t})$.\n",
    "\n",
    "- Fitting the $\\hat{\\beta}_{t-1}$ using all past user data:\n",
    "$$\n",
    "\\hat{\\beta}_{t-1} = \\big[ \\hat{\\beta}_{t-1,0}, \\hat{\\beta}_{t-1,1} \\big] = \\mathrm{argmin}_{\\beta} \\sum_{i=1}^n \\sum_{t'=1}^{t-1} \\left( R_{i,t'+1} - \\phi_0(S_{t'})^\\top \\beta_0 - A_{i,t'} \\phi_1(S_{i,t'})^\\top \\beta_1 \\right)^2\n",
    "$$\n",
    "\n",
    "- Form action selection probabilities\n",
    "$$\n",
    "\\mathbb{P}(A_{i,t} = 1 | H_{1:n,t}, S_{i,t}) = \\pi_\\min + (1-2\\pi_\\min) * \\left[ 1 + \\exp \\left( - b_{\\mathrm{steep}} \\phi_1(S_{i,t})^\\top \\hat{\\beta}_{t-1,1} \\right) \\right]^{-1}\n",
    "$$\n",
    "Generalized logistic function with $b_{\\mathrm{steep}}$ as hyperparameter.\n",
    "Also, $\\pi_\\min$ constrains action selection probabilities so for $\\pi_{\\min} = 0.1$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(A_{i,t} = 1 | H_{1:n,t}, S_{i,t}) \\in [\\pi_{\\min}, 1 - \\pi_{\\min}] ~~~~ \\mathrm{with~probability}~1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feeeedf",
   "metadata": {},
   "source": [
    "### Pooling Boltzmann Sampler: Algorithm Statistics\n",
    "\n",
    "__Estimating Equation Formulation:__ $\\hat{\\beta}_t$ solves $0 = \\frac{1}{n} \\sum_{i=1}^n \\dot{g}_{i,t}(\\hat{\\beta}_t)$\n",
    "where \n",
    "$$\\dot{g}_{i,t}(\\beta_t) \\triangleq \\sum_{t'=1}^t \\left( R_{i,t'+1} - \\phi_0(S_{t'})^\\top \\beta_0 - A_{i,t'} \\phi_1(S_{i,t'})^\\top \\beta_1 \\right) \\begin{bmatrix} \\phi_0(S_{t'}) \\\\ A_{i,t'} \\phi_1(S_{i,t'}) \\end{bmatrix}$$\n",
    "\n",
    "__Hessian:__\n",
    "$$\\ddot{G}_t \\triangleq - \\sum_{t'=1}^t \\begin{bmatrix} \\phi_0(S_{t'}) \\\\ A_{i,t'} \\phi_1(S_{i,t'}) \\end{bmatrix}^{\\otimes 2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingBoltzmanSampler:\n",
    "    \n",
    "    def __init__(self, n, pi_min, b_steep=1):\n",
    "        self.pi_min = pi_min\n",
    "        self.b_steep = b_steep\n",
    "        \n",
    "        self.all_policies = [ {\n",
    "            \"policy_num\": 0,\n",
    "            \"R_vec\": None,\n",
    "            \"feat_matrix\": None,\n",
    "            \"beta_est\": None,\n",
    "            \"user_ids\": None,\n",
    "        } ]\n",
    "        self.all_decisions = {}\n",
    "      \n",
    "    \n",
    "    def form_action1_prob(self, beta_est, treat_feats):\n",
    "        lin_est = np.dot(beta_est[-2:], treat_feats)\n",
    "        prob = self.pi_min + (1-2*self.pi_min) / ((1 + np.exp(-self.b_steep * lin_est)))\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    \n",
    "    def select_action(self, state, user_id):\n",
    "    \n",
    "        # Posterior Mean and Variance for beta_1 (treatment effect)\n",
    "        treat_feats = np.array([state[0], state[2]]).T\n",
    "        beta_est = self.all_policies[-1]['beta_est']\n",
    "        \n",
    "        if beta_est is None:\n",
    "            action1_prob = 0.5\n",
    "            pi_grad = None\n",
    "        else:\n",
    "            action1_prob = self.form_action1_prob(beta_est, treat_feats)\n",
    "            epsilon = np.sqrt(np.finfo(float).eps)\n",
    "            pi_grad = sp.optimize.approx_fprime(beta_est, \n",
    "                                            self.form_action1_prob, \n",
    "                                            epsilon,\n",
    "                                            treat_feats)\n",
    "            \n",
    "        action = np.random.binomial(1, action1_prob)\n",
    "            \n",
    "        policy_num = self.all_policies[-1]['policy_num']\n",
    "        if policy_num in self.all_decisions.keys():\n",
    "            self.all_decisions[policy_num][\"all_user_ids\"].append( user_id )\n",
    "            self.all_decisions[policy_num][\"all_treat_feats\"].append( treat_feats )\n",
    "            self.all_decisions[policy_num][\"all_action1_probs\"].append( action1_prob )\n",
    "            self.all_decisions[policy_num][\"all_actions\"].append( action )\n",
    "            self.all_decisions[policy_num][\"all_pi_grad\"].append( pi_grad )\n",
    "        else:\n",
    "            self.all_decisions[policy_num] = { \n",
    "                \"policy_num\": policy_num,\n",
    "                \"beta_est\": beta_est,\n",
    "                \"all_user_ids\": [user_id],\n",
    "                \"all_treat_feats\": [treat_feats],\n",
    "                \"all_action1_probs\": [action1_prob],\n",
    "                \"all_pi_grad\": [pi_grad],\n",
    "                \"all_actions\": [action],\n",
    "            }\n",
    "    \n",
    "        return action, action1_prob\n",
    "    \n",
    "\n",
    "    def update_algorithm(self, new_states, new_actions, new_rewards, new_user_ids):\n",
    "        alg_info = self.all_policies[-1]\n",
    "        \n",
    "        prev_R_vec = alg_info['R_vec']\n",
    "        prev_feat_matrix = alg_info['feat_matrix']\n",
    "        prev_user_ids = alg_info['user_ids']\n",
    "        \n",
    "        feat_matrix = np.array([new_states[:,0], new_states[:,2], \n",
    "                           new_actions, new_actions*new_states[:,2]]).T\n",
    "        rewards = np.expand_dims(new_rewards, 1)\n",
    "        \n",
    "        if prev_R_vec is None:\n",
    "            new_R_vec = rewards\n",
    "        else:\n",
    "            new_R_vec = np.concatenate([prev_R_vec.copy(), rewards], axis=0)\n",
    "            \n",
    "        if prev_feat_matrix is None:\n",
    "            new_feat_matrix = feat_matrix\n",
    "        else:\n",
    "            new_feat_matrix = np.concatenate([prev_feat_matrix.copy(), feat_matrix], axis = 0)\n",
    "            \n",
    "        if prev_user_ids is None:\n",
    "            all_user_ids = new_user_ids\n",
    "        else:\n",
    "            all_user_ids = np.concatenate([prev_user_ids.copy(), new_user_ids], axis = 0)\n",
    "        \n",
    "        RX = np.sum(new_feat_matrix*new_R_vec, 0)\n",
    "        XX = np.einsum( 'ij,ik->jk', new_feat_matrix, new_feat_matrix )\n",
    "        inv_XX = np.linalg.inv( XX )\n",
    "        beta_est = np.matmul(inv_XX, RX.reshape(-1))\n",
    "        \n",
    "        # ALGORITHM STATISTICS -----------------------------------------\n",
    "        \n",
    "        # Forming estimating equations for beta_est\n",
    "        residuals = new_R_vec.squeeze() - np.matmul(new_feat_matrix, beta_est)\n",
    "        residuals = np.expand_dims(residuals, 1)\n",
    "        raw_est_eqns = residuals * new_feat_matrix\n",
    "        _, est_eqns = npi.group_by(all_user_ids).sum(raw_est_eqns)\n",
    "        \n",
    "        # Forming hessian for beta_est\n",
    "        alg_hessian = - np.einsum('ij,ik->jk', new_feat_matrix, new_feat_matrix) / n\n",
    "            \n",
    "        new_alg_info = {\n",
    "                        \"policy_num\": alg_info['policy_num']+1,\n",
    "                        \"R_vec\": new_R_vec,\n",
    "                        \"feat_matrix\": new_feat_matrix,\n",
    "                        \"beta_est\": beta_est,\n",
    "                        \"est_eqns\": est_eqns,\n",
    "                        \"user_ids\": all_user_ids,\n",
    "                        \"hessian\": alg_hessian,\n",
    "                        }\n",
    "        self.all_policies.append(new_alg_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84994f4",
   "metadata": {},
   "source": [
    "### Preparing the MRT Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_empty_study_df(T, n):\n",
    "    dataset_rownames = [\"user_id\", \"decision_t\", \"reward\", \n",
    "                        \"action\", \"action1_prob\", \"intercept\", \"binary\", \"continuous\", \"dosage\"]\n",
    "\n",
    "    # Fill in user_ids and decision times\n",
    "    user_id_all = np.repeat([i for i in range(1,n+1)], T)\n",
    "    decision_t_all = np.tile([i for i in range(1,T+1)], n)\n",
    "    empty_cols = np.empty((n*T, len(dataset_rownames)-2))\n",
    "    empty_cols[:] = np.nan\n",
    "\n",
    "    # Make dataframe to record study data\n",
    "    dataset_entries = np.hstack( [ np.stack([user_id_all, decision_t_all]).T, empty_cols ] )\n",
    "    study_df = pd.DataFrame(dataset_entries, columns=dataset_rownames)\n",
    "\n",
    "    # Change type of columns\n",
    "    type_dict = {\n",
    "        'user_id': int,\n",
    "        'decision_t': int,\n",
    "    }\n",
    "    study_df = study_df.astype(type_dict)\n",
    "    \n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0719164",
   "metadata": {},
   "source": [
    "### Simulating the MRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_MRT(T, n, env_params, RL_alg):\n",
    "    study_df = make_empty_study_df(T, n)\n",
    "    \n",
    "    # Loop over decision times\n",
    "    for t in range(1,T+1):\n",
    "    \n",
    "        # Loop over users in the study\n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_action1_probs = []\n",
    "        all_rewards = []\n",
    "        for user_id in range(1,n+1):\n",
    "        \n",
    "            # Generate state\n",
    "            if t == 1:\n",
    "                state = generate_state(None, None, None, env_params)\n",
    "            else:\n",
    "                state = generate_state(prev_states[user_id-1], \n",
    "                                    prev_actions[user_id-1], \n",
    "                                    prev_rewards[user_id-1], env_params)\n",
    "        \n",
    "            # Form action selection probabilities\n",
    "            action, action1_prob = RL_alg.select_action(state, user_id)\n",
    "            \n",
    "        \n",
    "            # Generate reward\n",
    "            reward = generate_reward(state, action, env_params)\n",
    "        \n",
    "            # Record data\n",
    "            all_states.append(state)\n",
    "            all_actions.append(action)\n",
    "            all_action1_probs.append(action1_prob)\n",
    "            all_rewards.append(reward)\n",
    "    \n",
    "        # Save all user data\n",
    "        all_states = np.array(all_states)\n",
    "        all_actions = np.array(all_actions)\n",
    "        all_action1_probs = np.array(all_action1_probs)\n",
    "        all_rewards = np.array(all_rewards)\n",
    "        \n",
    "        # Update Algorithm\n",
    "        RL_alg.update_algorithm(new_states = all_states, \n",
    "                     new_actions = all_actions, \n",
    "                     new_rewards = all_rewards, \n",
    "                     new_user_ids = np.arange(1,n+1))\n",
    "    \n",
    "        idx_t = study_df.index[study_df['decision_t'] == t]\n",
    "        half_row_data = np.vstack([all_rewards, all_actions, all_action1_probs]).T\n",
    "        row_data = np.hstack([half_row_data, all_states])      \n",
    "        study_df.iloc[idx_t,2:] = row_data\n",
    "        \n",
    "        # Prepare for next decision time\n",
    "        prev_states = all_states\n",
    "        prev_actions = all_actions\n",
    "        prev_rewards = all_rewards\n",
    "    \n",
    "\n",
    "    type_dict = {\n",
    "        'action': int,\n",
    "        'intercept': int,\n",
    "        'binary': int,\n",
    "    }\n",
    "    study_df = study_df.astype(type_dict)\n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6fdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "n = 100\n",
    "pi_min = 0.1\n",
    "b_steep = 1\n",
    "\n",
    "# Form Decision Making Policy and Run MRT Study\n",
    "RL_alg = PoolingBoltzmanSampler(n=n, pi_min=pi_min, b_steep=b_steep)\n",
    "study_df = simulate_MRT(T, n, env_params, RL_alg)\n",
    "\n",
    "# Print first 10 rows of dataframe\n",
    "print(\"Average Treatment Prob: {}\".format(np.mean(study_df['action1_prob'])))\n",
    "print(\"\")\n",
    "study_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024acd8",
   "metadata": {},
   "source": [
    "# Part 2: Analyze Pooling RL MRT Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d543b749",
   "metadata": {},
   "source": [
    "## Estimating Causal Excursion Effect \n",
    "$$\n",
    "\\theta^\\star = \\mathbb{E}_{\\pi^\\star} \\left[ Y_{t+1}( \\bar A_{t-1}, 1 ) - Y_{t+1}( \\bar A_{t-1}, 0 ) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\mathrm{argmin_{(\\theta_0, \\theta_1) \\in \\mathbb{R}^2}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^T \\left( Y_{i,t+1} - \\psi(H_{i,t-1}, O_{i,t})^\\top \\theta_0 - A_{i,t} \\theta_1 \\right)^2 \\right\\}\n",
    "$$\n",
    "where $\\psi(H_{i,t-1}, O_{i,t}) = [1, C_{i,t}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf14fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_feat_matrix(study_df, base_feat_names, treat_feat_names):\n",
    "    base_feats = np.vstack([study_df[feat] for feat in base_feat_names]).T\n",
    "    treat_feats = np.vstack([study_df[feat] for feat in treat_feat_names]).T\n",
    "    actions = actions = study_df['action']\n",
    "\n",
    "    actions = np.expand_dims(actions, 1)\n",
    "    feat_matrix = np.concatenate([ base_feats, actions*treat_feats], axis=1)\n",
    "        \n",
    "    return feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcba11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Least Squares Estimator\n",
    "Y_vec = study_df['reward'].to_numpy()\n",
    "all_user_ids = study_df['user_id'].to_numpy()\n",
    "feat_matrix = form_feat_matrix(study_df, \n",
    "                               base_feat_names = [\"intercept\", \"continuous\"],\n",
    "                               treat_feat_names = [\"intercept\"])\n",
    "\n",
    "# Fit Linear Model\n",
    "reg = LinearRegression().fit(feat_matrix, Y_vec)\n",
    "thetahat = reg.coef_.copy()\n",
    "thetahat[0] = reg.intercept_\n",
    "\n",
    "print(\"Estimated excursion effect (theta_1):\")\n",
    "print( thetahat[-1:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3b1d91",
   "metadata": {},
   "source": [
    "## Estimating Variance via Standard Sandwich Variance\n",
    "\n",
    "### Standard Sandwich Variance Estimator: $\\big( \\ddot{L}^{(n)} \\big)^{-1} \\Sigma^{(n)} \\big( \\ddot{L}^{(n)} \\big)^{-1, \\top}$\n",
    "\n",
    "- $\\ddot{L}^{(n)}$ is an estimate of the Hessian or \"bread\" part of the sandwich variance:\n",
    "$$\n",
    "\\ddot{L}^{(n)} \\triangleq - \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^T \\begin{bmatrix} \\psi(H_{i,t-1}, O_{i,t}) \\\\ A_{i,t} \\end{bmatrix}^{\\otimes 2}\n",
    "$$\n",
    "(we use the notation $x^{\\otimes 2} \\triangleq x x^\\top$ for any vector $x$)\n",
    "\n",
    "- $\\hat{\\Sigma}^{(n)}$ is an estimate of the \"meat\" part of the sandwich variance:\n",
    "$$\n",
    "\\hat{\\Sigma}^{(n)} \\triangleq \\frac{1}{n} \\sum_{i=1}^n \\dot{\\ell}_i(\\hat\\theta)^{\\otimes 2}\n",
    "$$\n",
    "where $\\dot{\\ell}_i(\\theta) \\triangleq \\sum_{t=1}^T \\left( Y_{i,t+1} - \\psi(H_{i,t-1}, O_{i,t})^\\top \\theta_0 - A_{i,t} \\theta_1 \\right) \\begin{bmatrix} \\psi(H_{i,t-1}, O_{i,t}) \\\\ A_{i,t} \\end{bmatrix}$\n",
    "\n",
    "Above, we use $\\psi(H_{i,t-1}, O_{i,t}) = [1, C_{i,t}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea455bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_est_eqns(feat_matrix, thetahat, Y_vec, all_user_ids):\n",
    "    residuals = Y_vec - np.matmul(feat_matrix, thetahat)\n",
    "    residuals = np.expand_dims(residuals, 1)\n",
    "    raw_est_eqns = residuals * feat_matrix\n",
    "    est_eqns = npi.group_by(all_user_ids).sum(raw_est_eqns)\n",
    "    return est_eqns\n",
    "\n",
    "\n",
    "def least_squares_hessian(feat_matrix):\n",
    "    hessian = -np.einsum('ij,ik->ijk', feat_matrix, feat_matrix).mean(0)\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ce302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sandwich_variance(est_eqns, hessian, thetahat):\n",
    "    meat = np.einsum( 'ij,ik->jk', est_eqns, est_eqns )\n",
    "    meat = meat / n\n",
    "\n",
    "    inv_hessian = np.linalg.inv( hessian )\n",
    "    sandwich_var = np.matmul( np.matmul(inv_hessian, meat), inv_hessian )\n",
    "    sandwich_var = sandwich_var / n\n",
    "    return sandwich_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cce04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, theta_est_eqns = least_squares_est_eqns(feat_matrix, thetahat, Y_vec, all_user_ids)\n",
    "theta_hessian = least_squares_hessian(feat_matrix)\n",
    "sandwich_var = get_sandwich_variance(theta_est_eqns, theta_hessian, thetahat)\n",
    "\n",
    "print(\"Estimated sandwich variance for theta_1:\")\n",
    "print( sandwich_var[-1][-1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8ee21",
   "metadata": {},
   "source": [
    "## Estimating Variance via Adaptive Sandwich Variance\n",
    "\n",
    "### Adaptive Sandwich Variance Estimator: $\\big( \\ddot{L}^{(n)} \\big)^{-1} \\Sigma_{\\mathrm{adapt}}^{(n)} \\big( \\ddot{L}^{(n)} \\big)^{-1, \\top}$\n",
    "\n",
    "Note that $\\big( \\ddot{L}^{(n)} \\big)^{-1} \\Sigma_{\\mathrm{adapt}}^{(n)} \\big( \\ddot{L}^{(n)} \\big)^{-1, \\top}$ is equivalent to the bottom right block of the matrix \n",
    "$$\\begin{bmatrix} \\ddot{G}_{1:T-1}^{(n)} & 0 \\\\\n",
    "V_{1:T-1}^{(n)} & \\ddot{L}^{(n)} \\end{bmatrix}^{-1} \\Sigma_{1:T}^{(n)} \\begin{bmatrix} \\ddot{G}_{1:T-1}^{(n)} & 0 \\\\\n",
    "V_{1:T-1}^{(n)} & \\ddot{L}^{(n)} \\end{bmatrix}^{-1, \\top}$$\n",
    "where\n",
    "$\\Sigma_{1:T}^{(n)}$ is an estimate of the \"stacked meat\" part of the adaptive sandwich variance:\n",
    "$$\n",
    "\\Sigma_{1:T}^{(n)} \\triangleq \\frac{1}{n} \\sum_{i=1}^n \\begin{pmatrix} \\dot{g}_{i,1}(\\hat\\beta_1) \\\\\n",
    "\\dot{g}_{i,2}(\\hat\\beta_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\dot{g}_{i,T-1}(\\hat\\beta_{T-1}) \\\\\n",
    "\\dot{\\ell}_i(\\hat{\\theta}) \\end{pmatrix}^{\\otimes 2}\n",
    "$$\n",
    "where $\\dot{\\ell}_i(\\theta) \\triangleq \\sum_{t=1}^T \\left( Y_{i,t+1} - \\psi(H_{i,t-1}, O_{i,t})^\\top \\theta_0 - A_{i,t} \\theta_1 \\right) \\begin{bmatrix} \\psi(H_{i,t-1}, O_{i,t}) \\\\ A_{i,t} \\end{bmatrix}$ and\n",
    "$\\dot{g}_{i,t}(\\beta_t) \\triangleq \\sum_{t'=1}^t \\left( R_{i,t'+1} - \\phi_0(S_{t'})^\\top \\beta_0 - A_{i,t'} \\phi_1(S_{i,t'})^\\top \\beta_1 \\right) \\begin{bmatrix} \\phi_0(S_{t'}) \\\\ A_{i,t'} \\phi_1(S_{i,t'}) \\end{bmatrix}$\n",
    "\n",
    "(we use the notation $x^{\\otimes 2} \\triangleq x x^\\top$ for any vector $x$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63678f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_est_eqns(RL_alg, theta_est_eqns):\n",
    "\n",
    "    all_est_eqns = []\n",
    "    for policy_dict in RL_alg.all_policies:\n",
    "        policy_num = policy_dict['policy_num']\n",
    "        if policy_num in [0, T]:\n",
    "            continue\n",
    "        tmp_est_eqns = RL_alg.all_policies[1]['est_eqns']\n",
    "        all_est_eqns.append(tmp_est_eqns)\n",
    "\n",
    "    all_est_eqns.append(theta_est_eqns)\n",
    "    stacked_est_eqns = np.concatenate(all_est_eqns, axis=1)\n",
    "    \n",
    "    return stacked_est_eqns\n",
    "\n",
    "\n",
    "def get_weight_derivates(RL_alg):\n",
    "    all_W_grads = {}\n",
    "    \n",
    "    for policy_num in RL_alg.all_decisions.keys():\n",
    "        if policy_num == 0:\n",
    "            continue\n",
    "\n",
    "        tmp_pi_grad = np.array(RL_alg.all_decisions[policy_num]['all_pi_grad'])\n",
    "        tmp_actions = np.array(RL_alg.all_decisions[policy_num]['all_actions'])\n",
    "        tmp_action1_probs = np.array(RL_alg.all_decisions[policy_num]['all_action1_probs'])\n",
    "\n",
    "        tmp_actions = np.expand_dims(tmp_actions, 1)\n",
    "        tmp_action1_probs = np.expand_dims(tmp_action1_probs, 1)\n",
    "\n",
    "        W_grads = tmp_pi_grad * tmp_actions / tmp_action1_probs \\\n",
    "                    - tmp_pi_grad * (1-tmp_actions) / (1-tmp_action1_probs)\n",
    "    \n",
    "        all_W_grads[policy_num] = W_grads\n",
    "    \n",
    "    return all_W_grads\n",
    "\n",
    "def get_stacked_hessian(RL_alg, theta_est_eqns, theta_hessian):\n",
    "    # Collect Hessians (Block Diagonal)\n",
    "    alg_hessian = []\n",
    "    for policy_dict in RL_alg.all_policies:\n",
    "        policy_num = policy_dict['policy_num']\n",
    "        if policy_num in [0, T]:\n",
    "            continue\n",
    "        hessian = RL_alg.all_policies[policy_num]['hessian']\n",
    "        alg_hessian.append(hessian)\n",
    "\n",
    "    # Get Estimating Equations\n",
    "    stacked_est_eqns = get_stacked_est_eqns(RL_alg, theta_est_eqns)\n",
    "\n",
    "    # Outer produt of W grads and estimating equations\n",
    "    all_W_grads = get_weight_derivates(RL_alg)\n",
    "    all_columns = []\n",
    "    for policy_num in all_W_grads.keys():\n",
    "        col = np.einsum('ij,ik->jk', stacked_est_eqns, all_W_grads[policy_num]) / n\n",
    "        all_columns.append( col )\n",
    "\n",
    "    # Form Stacked hessian\n",
    "    beta_dim = alg_hessian[0].shape[0]\n",
    "    theta_dim = theta_hessian.shape[0]\n",
    "\n",
    "    stacked_hessian_col = []\n",
    "    for policy_dict in RL_alg.all_policies:\n",
    "        policy_num = policy_dict['policy_num']\n",
    "        if policy_num in [0, T]:\n",
    "            continue\n",
    "        zeros = np.zeros(((policy_num-1)*beta_dim, beta_dim))\n",
    "        tmp_hessian = alg_hessian[policy_num-1]\n",
    "        tmp_col = np.concatenate([ zeros, tmp_hessian, all_columns[policy_num-1][policy_num*beta_dim:] ], axis=0 )\n",
    "        stacked_hessian_col.append( tmp_col )\n",
    "    \n",
    "    zeros = np.zeros(((T-1)*beta_dim, theta_dim))\n",
    "    last_col = np.concatenate([zeros, theta_hessian])\n",
    "    stacked_hessian_col.append(last_col)\n",
    "    stacked_hessian = np.hstack(stacked_hessian_col)\n",
    "    \n",
    "    return stacked_hessian\n",
    "\n",
    "def get_adaptive_sandwich_variance(stacked_est_eqns, stacked_hessian):\n",
    "    inv_stacked_hessian = np.linalg.inv( stacked_hessian )\n",
    "    stacked_meat = np.einsum('ij,ik->jk', stacked_est_eqns, stacked_est_eqns) / n\n",
    "    stacked_sandwich = np.matmul( np.matmul(inv_stacked_hessian, stacked_meat), inv_stacked_hessian.T ) / n\n",
    "    return stacked_sandwich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69794feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Estimating Equations\n",
    "stacked_est_eqns = get_stacked_est_eqns(RL_alg, theta_est_eqns)\n",
    "stacked_hessian = get_stacked_hessian(RL_alg, theta_est_eqns, theta_hessian)\n",
    "\n",
    "stacked_sandwich = get_adaptive_sandwich_variance(stacked_est_eqns, stacked_hessian)\n",
    "\n",
    "print(\"Estimated adaptive sandwich variance for theta_1:\")\n",
    "print( stacked_sandwich[-1,-1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d0cf7",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "(1) Compare Standard Sandwich and Adaptive Sandwich Variance Estimators\n",
    "\n",
    "(2) Examine the `b_steep` argument Boltzmann sampling hyperparameter\n",
    "    $$\\mathbb{P}(A_{i,t} = 1 | H_{1:n,t}, S_{i,t}) = \\pi_\\min + (1-2\\pi_\\min) * \\left[ 1 + \\exp \\left( - b_{\\mathrm{steep}} \\phi_1(S_{i,t})^\\top \\hat{\\beta}_{t-1,1} \\right) \\right]^{-1}$$\n",
    "    \n",
    "   (2a) How does changing $b_{\\mathrm{steep}} > 0$ change the action selection probabilities? Can make a plot.\n",
    "    \n",
    "   (2b) How does changing `b_steep` affect the sandwich and adaptive sandwich variance estimators? For example, compare `b_steep = 5` vs `b_steep = 1`\n",
    "    \n",
    "(3) Examine the Posterior Sampling and epsilon-greedy algorithms you looked at earlier in Raaz's coding session. Show that these algorithms do not converge to a limiting policy when the treatment effect (or margin) is zero. Compare this to using a Botlzmann sampling algorithm\n",
    "\n",
    "\n",
    "__Optional / Additional Exercises:__\n",
    "- Use the sandwich and adaptive sandwich variances to form 95% confidence intervals for $\\theta_1^\\star$\n",
    "- How does changing the data generating environment affect the sandwich and adaptive sandwich variance estimators?\n",
    "- How does changing the model used to form $\\theta$ affect the sandwich and adaptive sandwich variance estimators? Can look at excursion effects conditioned on state.\n",
    "- Compare to the individual RL algorithm to personalize, as used in the Monday afternoon exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
