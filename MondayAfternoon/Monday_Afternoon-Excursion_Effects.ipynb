{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7120cbf1",
   "metadata": {},
   "source": [
    "# Monday Afternoon: Excursion Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86413ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c72e9",
   "metadata": {},
   "source": [
    "## Simple Data Generating Environment\n",
    "- \"Covariates\": $X_t = (1, B_t, C_t, D_t)$ where $B_t$ is binary and $C_t$ is continuous and $D_t$ is a measure of \"dosage\"\n",
    "    - Specifically $D_t = \\frac{1}{1-\\gamma} \\sum_{t'=1}^{t-1} \\gamma^{t'} A_{t'}$ for $\\gamma = 0.95$. We normalize by $1-\\gamma$ to ensure $D_t \\in [0,1]$.\n",
    "- Binary action $A_t \\in \\{0, 1\\}$\n",
    "- Rewards are generated as follows:\n",
    "\n",
    "$$\n",
    "R_{t+1} = f_0(X_t)^\\top \\alpha_0 + A_t f_1(X_t)^\\top \\alpha_1 + \\epsilon_t\n",
    "$$\n",
    "\n",
    "where $f_0(X_t) = (1, B_t, C_t, D_t)$, $f_1(X_t) = (1, C_t)$, \n",
    "\n",
    "\n",
    "$\\alpha_0 = (1, -0.8, 0.05, -0.7)$, $\\alpha_1 = (0.3, 0)$, $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_{\\mathrm{env}}^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_params = {\n",
    "    \"alpha0\": np.array([1, 0.5, 0.2, -0.7]),\n",
    "    \"alpha1\": np.array([0.3, 0.05]),\n",
    "    \"sigma_env\": 0.5,\n",
    "    \"B_p\": 0.3,\n",
    "}\n",
    "\n",
    "def generate_reward(state, action, env_params):\n",
    "    base_state = np.array([1, state[1], state[2], state[3]])\n",
    "    treat_state = np.array([1, state[2]])\n",
    "    \n",
    "    base_mean_reward = np.dot( base_state, env_params[\"alpha0\"] )\n",
    "    mean_reward = base_mean_reward + \\\n",
    "                    action * np.dot( treat_state, env_params[\"alpha1\"] )\n",
    "    reward = mean_reward + np.random.normal(scale=env_params[\"sigma_env\"])\n",
    "    return reward\n",
    "        \n",
    "\n",
    "def generate_state(prev_state, prev_action, prev_reward, env_params):\n",
    "    B = np.random.binomial(1, env_params[\"B_p\"])\n",
    "    C = np.random.normal(3, scale=1)\n",
    "    \n",
    "    # Form dosage update\n",
    "    gamma = 0.95\n",
    "    norm_gamma = 1/(1-gamma)\n",
    "    if prev_state is None:\n",
    "        dosage = np.random.binomial(1, 0.5)\n",
    "    else:\n",
    "        dosage = (gamma*norm_gamma*prev_state[-1] + prev_action ) / norm_gamma\n",
    "    \n",
    "    state = np.array([1, B, C, dosage])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48802a1",
   "metadata": {},
   "source": [
    "## Simple Randomization Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac2429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "      \n",
    "    \n",
    "    def form_action1_prob(self, state, user_id):\n",
    "        # State variables order: \"intercept\", \"binary\", \"continuous\"\n",
    "        \n",
    "        if state[1] == 1: # binary state variable\n",
    "            return 0.3\n",
    "        else:\n",
    "            return 0.5\n",
    "        \n",
    "    def update_algorithm(self, new_states, new_actions, new_rewards, all_user_ids):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690446cf",
   "metadata": {},
   "source": [
    "### Preparing the MRT Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908beb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_empty_study_df(T, n):\n",
    "    dataset_rownames = [\"user_id\", \"decision_t\", \"reward\", \n",
    "                        \"action\", \"action1_prob\", \"intercept\", \"binary\", \"continuous\", \"dosage\"]\n",
    "\n",
    "    # Fill in user_ids and decision times\n",
    "    user_id_all = np.repeat([i for i in range(1,n+1)], T)\n",
    "    decision_t_all = np.tile([i for i in range(1,T+1)], n)\n",
    "    empty_cols = np.empty((n*T, len(dataset_rownames)-2))\n",
    "    empty_cols[:] = np.nan\n",
    "\n",
    "    # Make dataframe to record study data\n",
    "    dataset_entries = np.hstack( [ np.stack([user_id_all, decision_t_all]).T, empty_cols ] )\n",
    "    study_df = pd.DataFrame(dataset_entries, columns=dataset_rownames)\n",
    "\n",
    "    # Change type of columns\n",
    "    type_dict = {\n",
    "        'user_id': int,\n",
    "        'decision_t': int,\n",
    "    }\n",
    "    study_df = study_df.astype(type_dict)\n",
    "    \n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a127f",
   "metadata": {},
   "source": [
    "### Simulating the MRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_MRT(T, n, env_params, RL_alg):\n",
    "    study_df = make_empty_study_df(T, n)\n",
    "    \n",
    "    # Loop over decision times\n",
    "    for t in range(1,T+1):\n",
    "    \n",
    "        # Loop over users in the study\n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_action1_probs = []\n",
    "        all_rewards = []\n",
    "        for user_id in range(1,n+1):\n",
    "        \n",
    "            # Generate state\n",
    "            if t == 1:\n",
    "                state = generate_state(None, None, None, env_params)\n",
    "            else:\n",
    "                state = generate_state(prev_states[user_id-1], \n",
    "                                    prev_actions[user_id-1], \n",
    "                                    prev_rewards[user_id-1], env_params)\n",
    "        \n",
    "            # Form action selection probabilities\n",
    "            action1_prob = RL_alg.form_action1_prob(state, user_id)\n",
    "            action = np.random.binomial(1, action1_prob)\n",
    "        \n",
    "            # Generate reward\n",
    "            reward = generate_reward(state, action, env_params)\n",
    "        \n",
    "            # Record data\n",
    "            all_states.append(state)\n",
    "            all_actions.append(action)\n",
    "            all_action1_probs.append(action1_prob)\n",
    "            all_rewards.append(reward)\n",
    "    \n",
    "        # Save all user data\n",
    "        all_states = np.array(all_states)\n",
    "        all_actions = np.array(all_actions)\n",
    "        all_action1_probs = np.array(all_action1_probs)\n",
    "        all_rewards = np.array(all_rewards)\n",
    "        \n",
    "        # Update Algorithm\n",
    "        RL_alg.update_algorithm(new_states = all_states, \n",
    "                     new_actions = all_actions, \n",
    "                     new_rewards = all_rewards, \n",
    "                     all_user_ids = np.arange(1,n+1))\n",
    "    \n",
    "        idx_t = study_df.index[study_df['decision_t'] == t]\n",
    "        half_row_data = np.vstack([all_rewards, all_actions, all_action1_probs]).T\n",
    "        row_data = np.hstack([half_row_data, all_states])      \n",
    "        study_df.iloc[idx_t,2:] = row_data\n",
    "        \n",
    "        # Prepare for next decision time\n",
    "        prev_states = all_states\n",
    "        prev_actions = all_actions\n",
    "        prev_rewards = all_rewards\n",
    "    \n",
    "\n",
    "    type_dict = {\n",
    "        'action': int,\n",
    "        'intercept': int,\n",
    "        'binary': int,\n",
    "    }\n",
    "    study_df = study_df.astype(type_dict)\n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0e7a2",
   "metadata": {},
   "source": [
    "### Simulate MRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b745df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 50\n",
    "n = 100\n",
    "\n",
    "# Form Decision Making Policy and Run MRT Study\n",
    "policy = SimplePolicy()\n",
    "study_df = simulate_MRT(T, n, env_params, policy)\n",
    "\n",
    "\n",
    "# Print first 10 rows of dataframe\n",
    "print(\"Average Treatment Prob: {}\".format(np.mean(study_df['action1_prob'])))\n",
    "print(\"\")\n",
    "study_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c7222",
   "metadata": {},
   "source": [
    "## (1) Estimating Causal Excursion Effect \n",
    "$$\n",
    "\\psi_1(X_{i,t})^\\top \\theta^\\star = \\mathbb{E} \\left[ Y_{t+1}( \\bar A_{t-1}, 1 ) - Y_{t+1}( \\bar A_{t-1}, 0 ) \\big| H_{i,t-1}, X_{i,t} \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\mathrm{argmin_{(\\theta_0, \\theta_1) \\in \\mathbb{R}^2}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^T \\left( Y_{i,t+1} - \\psi_0(H_{i,t-1}, X_{i,t})^\\top \\eta - (A_{i,t}-\\pi_{i,t}) \\psi_1(X_{i,t})^\\top \\theta \\right)^2 \\right\\}\n",
    "$$\n",
    "where $\\psi_0(H_{i,t-1}, X_{i,t}) = [1, B_{i,t}, C_{i,t}]$ and $\\psi_1(X_{i,t}) = [1, C_{i,t}]$. Recall that $X_{i,t} = (1, B_{i,t}, C_{i,t})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcf375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_feat_matrix(study_df, base_feat_names, treat_feat_names, \n",
    "                     action_centering=False):\n",
    "    \"\"\"\n",
    "    If action_centering = True\n",
    "        Return matrix where each row is [ g(H_{i,t-1}, X_{i,t}), (A_{i,t}-\\pi_{i,t}) f(X_{i,t}) ]\n",
    "    Else:\n",
    "        Return matrix where each row is [ g(H_{i,t-1}, X_{i,t}), A_{i,t} f(X_{i,t}) ]\n",
    "    \"\"\"\n",
    "    base_feats = np.vstack([study_df[feat] for feat in base_feat_names]).T\n",
    "    treat_feats = np.vstack([study_df[feat] for feat in treat_feat_names]).T\n",
    "    actions = actions = study_df['action']\n",
    "    action_probs = study_df['action1_prob']\n",
    "    \n",
    "    if action_centering:\n",
    "        \"\"\"\n",
    "        EXERCISE: Form features with action centering\n",
    "        \"\"\"\n",
    "        pass\n",
    "    else:\n",
    "        actions = np.expand_dims(actions, 1)\n",
    "        feat_matrix = np.concatenate([ base_feats, actions*treat_feats], axis=1)\n",
    "        \n",
    "    return feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a5023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_model(feat_matrix, Y_vec, weights=None):\n",
    "    reg = LinearRegression().fit(feat_matrix, Y_vec, weights)\n",
    "    thetahat = reg.coef_.copy()\n",
    "    thetahat[0] = reg.intercept_\n",
    "    return thetahat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Least Squares Estimator\n",
    "Y_vec = study_df['reward']\n",
    "feat_matrix = form_feat_matrix(study_df, \n",
    "                               base_feat_names = [\"intercept\", \"binary\", \"continuous\"],\n",
    "                               treat_feat_names = [\"intercept\", \"continuous\"],\n",
    "                               action_centering=True)\n",
    "\n",
    "# Fit Linear Model\n",
    "thetahat = fit_linear_model(feat_matrix, Y_vec)\n",
    "\n",
    "print(\"True excursion effect (theta):\")\n",
    "print( env_params['alpha1'] )\n",
    "print(\"Estimated excursion effect (theta):\")\n",
    "print( thetahat[-2:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4842a57",
   "metadata": {},
   "source": [
    "## (2) Investiagate the Impact of Action Centering\n",
    "$$\n",
    "\\hat{\\theta} = \\mathrm{argmin_{(\\theta_0, \\theta_1) \\in \\mathbb{R}^2}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^T \\left( Y_{i,t+1} - \\psi_0(H_{i,t-1}, X_{i,t})^\\top \\eta - (A_{i,t}-\\pi_{i,t}) \\psi_1(X_{i,t})^\\top \\theta \\right)^2 \\right\\}\n",
    "$$\n",
    "where $\\psi_0(H_{i,t-1}, X_{i,t}) = [1, B_{i,t}, C_{i,t}]$ and $\\psi_1(X_{i,t}) = [1, C_{i,t}]$. Recall that $X_{i,t} = (1, B_{i,t}, C_{i,t})$.\n",
    "\n",
    "__Questions:__\n",
    "- What happens if the for the average reward is very bad? For example $\\psi_0(H_{i,t-1}, X_{i,t}) = 1$?\n",
    "- How does this impact the estimation of the excursion effect theta? Try both with and without action centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Least Squares Estimator no action centering #################\n",
    "\"\"\"\n",
    "EXERCISE: Fit Linear Model without action centering\n",
    "\"\"\"\n",
    "thetahat_noac = np.nan(5)\n",
    "\n",
    "\n",
    "# Form Least Squares Estimator with action centering #################\n",
    "\"\"\"\n",
    "# EXERCISE: Fit Linear Model with action centering \n",
    "\"\"\"\n",
    "thetahat_ac = np.zeros(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"True excursion effect (theta):\")\n",
    "print( env_params['alpha1'] )\n",
    "print(\"Estimated excursion effect - no action centering (theta):\")\n",
    "print( thetahat_noac[-2:] )\n",
    "print(\"Estimated excursion effect - with action centering (theta):\")\n",
    "print( thetahat_ac[-2:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a5b19",
   "metadata": {},
   "source": [
    "## (3) Generate MRT Dataset with a Reinforcement Learning Algorithm\n",
    "- Use the following posterior sampling algorithm in a simulated MRT\n",
    "    - The algorithm must only learn using the data of a single user (individual RL algorithms).\n",
    "- Alternatively you can code your own RL Algorithm of choice (e.g. from those Raaz taught earlier)\n",
    "    - Make sure to restrict the action selection probabilities within [$\\pi_\\min, 1-\\pi_\\min$]\n",
    "    - Specifically, fill out the `form_action1_prob` and `update_algorithm` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb4aa1",
   "metadata": {},
   "source": [
    "### Example RL algorithm you could implement: Posterior Sampling\n",
    "- Normal priors and likelihoods\n",
    "- RL algorithm's model of the reward:\n",
    "\n",
    "$$\n",
    "R_{t+1} = \\phi_0(X_t)^\\top \\beta_0 + A_t \\phi_1(X_t)^\\top \\beta_1 + \\epsilon_t\n",
    "$$\n",
    "\n",
    "where $\\phi_0(X_t) = (1, C_t)$, $\\phi_1(X_t) = (1, C_t)$, and $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2_{\\mathrm{alg}})$.\n",
    "\n",
    "- Prior: \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    \\beta_0 \\\\ \n",
    "    \\beta_1\n",
    "\\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, I_4 \\right)\n",
    "$$\n",
    "\n",
    "- Constrain action selection probabilities so for $\\pi_{\\min} = 0.1$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(A_t = 1 | H_t, X_t) \\in [\\pi_{\\min}, 1 - \\pi_{\\min}] ~~~~ \\mathrm{with~probability}~1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorSampler:\n",
    "    \n",
    "    def __init__(self, n, prior_mean, prior_var, sigma_alg, pi_min):\n",
    "        self.sigma_alg = sigma_alg\n",
    "        self.pi_min = pi_min\n",
    "        \n",
    "        all_posteriors = {}\n",
    "        for user_id in range(1, n+1):\n",
    "            posterior = {\n",
    "                \"posterior_mean\": prior_mean.copy(),\n",
    "                \"posterior_var\": prior_var.copy(),\n",
    "            }\n",
    "            all_posteriors[user_id] = posterior\n",
    "            \n",
    "        self.all_posteriors = all_posteriors\n",
    "      \n",
    "    \n",
    "    def form_action1_prob(self, state, user_id):\n",
    "    \n",
    "        # Posterior Mean and Variance for beta_1 (treatment effect)\n",
    "        posterior = self.all_posteriors[user_id]\n",
    "        post_mean_beta1 = posterior['posterior_mean'][-1]\n",
    "        post_var_beta1 = posterior['posterior_var'][-1,-1]\n",
    "    \n",
    "        raw_prob = sp.stats.norm.cdf( post_mean_beta1 / np.sqrt(post_var_beta1) )\n",
    "    \n",
    "        # Restrict action selection probabilities to [pi_min, 1-pi_min]\n",
    "        return self.clip_action1_prob(raw_prob, self.pi_min)\n",
    "    \n",
    "    \n",
    "    def clip_action1_prob(self, raw_prob, pi_min):\n",
    "        clipped_prob = np.maximum(pi_min, raw_prob)\n",
    "        clipped_prob = np.minimum(1-pi_min, clipped_prob)\n",
    "        return clipped_prob\n",
    "        \n",
    "        \n",
    "    def update_algorithm(self, new_states, new_actions, new_rewards, all_user_ids):\n",
    "        \n",
    "        # update posterior for each user\n",
    "        for user_id in all_user_ids:\n",
    "            user_state = new_states[user_id-1]\n",
    "            user_action = new_actions[user_id-1]\n",
    "            user_reward = new_rewards[user_id-1]\n",
    "        \n",
    "            posterior = self.all_posteriors[user_id]\n",
    "            post_var = posterior['posterior_var']\n",
    "            post_mean = posterior['posterior_mean']\n",
    "            feat_vec = np.array([user_state[0], user_state[2], \n",
    "                                 user_action, user_reward*user_state[2]])\n",
    "\n",
    "            inv_post_var = np.linalg.inv(post_var)\n",
    "            new_inv_post_var = inv_post_var + np.outer(feat_vec, feat_vec) / self.sigma_alg\n",
    "            new_post_var = np.linalg.inv( new_inv_post_var )\n",
    "    \n",
    "            new_post_mean_num = np.matmul(inv_post_var, post_mean) + \\\n",
    "                                user_reward * feat_vec / self.sigma_alg\n",
    "            new_post_mean = np.matmul( new_post_var, new_post_mean_num )\n",
    "    \n",
    "            self.all_posteriors[user_id]['posterior_mean'] = new_post_mean\n",
    "            self.all_posteriors[user_id]['posterior_var'] = new_post_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAlg:\n",
    "    \n",
    "    def __init__(self, n, pi_min):\n",
    "        self.pi_min = pi_min\n",
    "        self.n\n",
    "        \n",
    "    \n",
    "    def form_action1_prob(self, state, user_id):\n",
    "        \"\"\"\n",
    "        TODO: form action selection probability for a given user and state\n",
    "        \"\"\"\n",
    "        # State variables order: \"intercept\", \"binary\", \"continuous\"\n",
    "        \n",
    "        if state[1] == 1: # binary state variable\n",
    "            raw_prob = 0.3\n",
    "        else:\n",
    "            raw_prob = 0.5\n",
    "    \n",
    "        # Restrict action selection probabilities to [pi_min, 1-pi_min]\n",
    "        return self.clip_action1_prob(raw_prob, self.pi_min)\n",
    "    \n",
    "    \n",
    "    def clip_action1_prob(self, raw_prob, pi_min):\n",
    "        \"\"\"\n",
    "        Constrain action selection probabilities between pi_min and 1-pi_min\n",
    "        \"\"\"\n",
    "        assert 0 < pi_min <= 0.5\n",
    "        clipped_prob = np.maximum(pi_min, raw_prob)\n",
    "        clipped_prob = np.minimum(1-pi_min, clipped_prob)\n",
    "        return clipped_prob\n",
    "        \n",
    "        \n",
    "    def update_algorithm(self, new_states, new_actions, new_rewards, all_user_ids):\n",
    "        \"\"\"\n",
    "        TODO: Update RL algorithm\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ace533",
   "metadata": {},
   "source": [
    "### Generate Dataset with RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL Algorithm Hyperparameters\n",
    "pi_min = 0.05\n",
    "prior_mean = np.array([0.5, 0, 0, 0])\n",
    "prior_var = np.eye(4)\n",
    "sigma_alg = 1\n",
    "\n",
    "\n",
    "# Form RL Algorithm and Run MRT Study\n",
    "RL_alg = PosteriorSampler(n, prior_mean, prior_var, sigma_alg, pi_min)\n",
    "study_df_RL = simulate_MRT(T, n, env_params, RL_alg)\n",
    "\n",
    "\n",
    "# Print first 10 rows of dataframe\n",
    "print(\"Average Treatment Prob: {}\".format(np.mean(study_df_RL['action1_prob'])))\n",
    "print(\"\")\n",
    "study_df_RL.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852d42d",
   "metadata": {},
   "source": [
    "## (4) Estimating Causal Excursion Effect on RL Data\n",
    "$$\n",
    "\\theta^\\star = \\mathbb{E} \\left[ Y_{t+1}( \\bar A_{t-1}, 1 ) - Y_{t+1}( \\bar A_{t-1}, 0 ) \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\mathrm{argmin_{(\\theta_0, \\theta_1) \\in \\mathbb{R}^2}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^T W_{i,t} \\left( Y_{i,t+1} - \\psi_0(H_{i,t-1}, X_{i,t})^\\top \\eta - (A_{i,t}-\\pi_{i,t}) \\psi_1(X_{i,t})^\\top \\theta \\right)^2 \\right\\}\n",
    "$$\n",
    "where $\\psi_0(H_{i,t-1}, X_{i,t}) = [1, B_{i,t}, C_{i,t}]$ and $\\psi_1(X_{i,t}) = [1, C_{i,t}]$. Recall that $X_{i,t} = (1, B_{i,t}, C_{i,t})$.\n",
    "\n",
    "__Make sure to include weights $W_{i,t}$!!__\n",
    "$$W_{i,t} = \\bigg( \\frac{p(X_{i,t})}{\\pi_{i,t}} \\bigg)^{A_{i,t}} \\bigg( \\frac{1-p(X_{i,t})}{1-\\pi_{i,t}} \\bigg)^{1-A_{i,t}}$$\n",
    "for some pre-specified policy $p$, to ensure that $p(X_{i,t})$ only depends on $X_{i,t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form Least Squares Estimator\n",
    "Y_vec = study_df['reward']\n",
    "feat_matrix = form_feat_matrix(study_df, \n",
    "                               base_feat_names = [\"intercept\", \"binary\", \"continuous\"],\n",
    "                               treat_feat_names = [\"intercept\", \"continuous\"],\n",
    "                               action_centering=True)\n",
    "\n",
    "\"\"\"\n",
    "EXERCISE: Form Vector of weights\n",
    "\"\"\"\n",
    "action1_probs = study_df_RL['action1_prob'].to_numpy()\n",
    "actions = study_df_RL['action'].to_numpy()\n",
    "weight_vec = np.ones(action1_probs.shape) # EXERCISE: change this to weights W_{i,t}\n",
    "thetahat = fit_linear_model(feat_matrix, Y_vec, weight_vec)\n",
    "\n",
    "print(\"True excursion effect (theta):\")\n",
    "print( env_params['alpha1'] )\n",
    "print(\"Estimated excursion effect (theta):\")\n",
    "print( thetahat[-2:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f459e4",
   "metadata": {},
   "source": [
    "## (5) Additional Exploration (on your own time)\n",
    "- What happens if you change $\\pi_\\min$? How do we expect our estimation of the excursion effect to change?\n",
    "- Try estimate a marginal excursion effect (not conditional on state)?\n",
    "- Estimate the limiting variance of the excursion effect and form a confidence interval via a normal approximation. For the variance formula see Proposition 3.1 in [Assessing Time-Varying Causal Effect Moderation in Mobile Health](https://arxiv.org/abs/1601.00237).\n",
    "- You could form a more complex reward generating model. In this setting, one could use a machine learning model (e.g. random forrest) to replace $\\psi_1(H_{i,t-1}, X_{i,t})^\\top \\eta$. the See [Double/Debiased Machine Learning for Treatment and Causal Parameters](https://arxiv.org/abs/1608.00060)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5b606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
