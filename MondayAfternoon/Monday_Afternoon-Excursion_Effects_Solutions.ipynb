{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7120cbf1",
   "metadata": {},
   "source": [
    "# Monday Afternoon: Excursion Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d86413ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c72e9",
   "metadata": {},
   "source": [
    "## Simple Data Generating Environment\n",
    "- \"Covariates\": $O_t = (1, B_t, C_t, D_t)$ where $B_t$ is binary and $C_t$ is continuous and $D_t$ is a measure of \"dosage\"\n",
    "    - Specifically $D_t = \\frac{1}{1-\\gamma} \\sum_{t'=1}^{t-1} \\gamma^{t'} A_{t'}$ for $\\gamma = 0.95$. We normalize by $1-\\gamma$ to ensure $D_t \\in [0,1]$.\n",
    "- Binary action $A_t \\in \\{0, 1\\}$\n",
    "- Rewards are generated as follows:\n",
    "\n",
    "$$\n",
    "R_{t+1} = f_0(O_t)^\\top \\alpha_0 + A_t f_1(O_t)^\\top \\alpha_1 + \\epsilon_t\n",
    "$$\n",
    "\n",
    "where $f_0(O_t) = (1, B_t, C_t, D_t)$, $f_1(O_t) = (1, C_t)$, \n",
    "\n",
    "\n",
    "$\\alpha_0 = (1, -0.8, 0.05, -0.7)$, $\\alpha_1 = (0.3, 0)$, $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_{\\mathrm{env}}^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa0d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_params = {\n",
    "    \"alpha0\": np.array([1, 0.5, 0.2, -0.7]),\n",
    "    \"alpha1\": np.array([0.3, 0.05]),\n",
    "    \"sigma_env\": 0.5,\n",
    "    \"B_p\": 0.3,\n",
    "}\n",
    "\n",
    "def generate_reward(state, action, env_params):\n",
    "    base_state = np.array([1, state[1], state[2], state[3]])\n",
    "    treat_state = np.array([1, state[2]])\n",
    "    \n",
    "    base_mean_reward = np.dot( base_state, env_params[\"alpha0\"] )\n",
    "    mean_reward = base_mean_reward + \\\n",
    "                    action * np.dot( treat_state, env_params[\"alpha1\"] )\n",
    "    reward = mean_reward + np.random.normal(scale=env_params[\"sigma_env\"])\n",
    "    return reward\n",
    "        \n",
    "\n",
    "def generate_state(prev_state, prev_action, prev_reward, env_params):\n",
    "    B = np.random.binomial(1, env_params[\"B_p\"])\n",
    "    C = np.random.normal(3, scale=1)\n",
    "    \n",
    "    # Form dosage update\n",
    "    gamma = 0.95\n",
    "    norm_gamma = 1/(1-gamma)\n",
    "    if prev_state is None:\n",
    "        dosage = np.random.binomial(1, 0.5)\n",
    "    else:\n",
    "        dosage = (gamma*norm_gamma*prev_state[-1] + prev_action ) / norm_gamma\n",
    "    \n",
    "    state = np.array([1, B, C, dosage])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48802a1",
   "metadata": {},
   "source": [
    "## Simple Randomization Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac2429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "      \n",
    "    \n",
    "    def form_action1_prob(self, state, user_id):\n",
    "        # State variables order: \"intercept\", \"binary\", \"continuous\"\n",
    "        \n",
    "        if state[1] == 1: # binary state variable\n",
    "            return 0.3\n",
    "        else:\n",
    "            return 0.5\n",
    "        \n",
    "    def update_algorithm(self, new_states, new_actions, new_rewards, all_user_ids):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690446cf",
   "metadata": {},
   "source": [
    "### Preparing the MRT Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908beb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_empty_study_df(T, n):\n",
    "    dataset_rownames = [\"user_id\", \"decision_t\", \"reward\", \n",
    "                        \"action\", \"action1_prob\", \"intercept\", \"binary\", \"continuous\", \"dosage\"]\n",
    "\n",
    "    # Fill in user_ids and decision times\n",
    "    user_id_all = np.repeat([i for i in range(1,n+1)], T)\n",
    "    decision_t_all = np.tile([i for i in range(1,T+1)], n)\n",
    "    empty_cols = np.empty((n*T, len(dataset_rownames)-2))\n",
    "    empty_cols[:] = np.nan\n",
    "\n",
    "    # Make dataframe to record study data\n",
    "    dataset_entries = np.hstack( [ np.stack([user_id_all, decision_t_all]).T, empty_cols ] )\n",
    "    study_df = pd.DataFrame(dataset_entries, columns=dataset_rownames)\n",
    "\n",
    "    # Change type of columns\n",
    "    type_dict = {\n",
    "        'user_id': int,\n",
    "        'decision_t': int,\n",
    "    }\n",
    "    study_df = study_df.astype(type_dict)\n",
    "    \n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6a127f",
   "metadata": {},
   "source": [
    "### Simulating the MRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27de8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_MRT(T, n, env_params, RL_alg):\n",
    "    study_df = make_empty_study_df(T, n)\n",
    "    \n",
    "    # Loop over decision times\n",
    "    for t in range(1,T+1):\n",
    "    \n",
    "        # Loop over users in the study\n",
    "        all_states = []\n",
    "        all_actions = []\n",
    "        all_action1_probs = []\n",
    "        all_rewards = []\n",
    "        for user_id in range(1,n+1):\n",
    "        \n",
    "            # Generate state\n",
    "            if t == 1:\n",
    "                state = generate_state(None, None, None, env_params)\n",
    "            else:\n",
    "                state = generate_state(prev_states[user_id-1], \n",
    "                                    prev_actions[user_id-1], \n",
    "                                    prev_rewards[user_id-1], env_params)\n",
    "        \n",
    "            # Form action selection probabilities\n",
    "            action1_prob = RL_alg.form_action1_prob(state, user_id)\n",
    "            action = np.random.binomial(1, action1_prob)\n",
    "        \n",
    "            # Generate reward\n",
    "            reward = generate_reward(state, action, env_params)\n",
    "        \n",
    "            # Record data\n",
    "            all_states.append(state)\n",
    "            all_actions.append(action)\n",
    "            all_action1_probs.append(action1_prob)\n",
    "            all_rewards.append(reward)\n",
    "    \n",
    "        # Save all user data\n",
    "        all_states = np.array(all_states)\n",
    "        all_actions = np.array(all_actions)\n",
    "        all_action1_probs = np.array(all_action1_probs)\n",
    "        all_rewards = np.array(all_rewards)\n",
    "        \n",
    "        # Update Algorithm\n",
    "        RL_alg.update_algorithm(new_states = all_states, \n",
    "                     new_actions = all_actions, \n",
    "                     new_rewards = all_rewards, \n",
    "                     all_user_ids = np.arange(1,n+1))\n",
    "    \n",
    "        idx_t = study_df.index[study_df['decision_t'] == t]\n",
    "        half_row_data = np.vstack([all_rewards, all_actions, all_action1_probs]).T\n",
    "        row_data = np.hstack([half_row_data, all_states])      \n",
    "        study_df.iloc[idx_t,2:] = row_data\n",
    "        \n",
    "        # Prepare for next decision time\n",
    "        prev_states = all_states\n",
    "        prev_actions = all_actions\n",
    "        prev_rewards = all_rewards\n",
    "    \n",
    "\n",
    "    type_dict = {\n",
    "        'action': int,\n",
    "        'intercept': int,\n",
    "        'binary': int,\n",
    "    }\n",
    "    study_df = study_df.astype(type_dict)\n",
    "    return study_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b0e7a2",
   "metadata": {},
   "source": [
    "### Simulate MRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b745df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Prob: 0.44011999999999996\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>decision_t</th>\n",
       "      <th>reward</th>\n",
       "      <th>action</th>\n",
       "      <th>action1_prob</th>\n",
       "      <th>intercept</th>\n",
       "      <th>binary</th>\n",
       "      <th>continuous</th>\n",
       "      <th>dosage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.215127</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.197827</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.417900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.377510</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.975926</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.832585</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.421218</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.994290</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.678544</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.139551</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.051982</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.941154</td>\n",
       "      <td>0.045125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1.499440</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.093228</td>\n",
       "      <td>0.042869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.497718</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.489415</td>\n",
       "      <td>0.040725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2.240632</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.209942</td>\n",
       "      <td>0.088689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2.611479</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.550368</td>\n",
       "      <td>0.084255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  decision_t    reward  action  action1_prob  intercept  binary  \\\n",
       "0        1           1  1.215127       0           0.5          1       0   \n",
       "1        1           2  2.417900       0           0.3          1       1   \n",
       "2        1           3  1.975926       1           0.5          1       0   \n",
       "3        1           4  1.421218       0           0.5          1       0   \n",
       "4        1           5  1.678544       0           0.5          1       0   \n",
       "5        1           6  1.051982       0           0.5          1       0   \n",
       "6        1           7  1.499440       0           0.5          1       0   \n",
       "7        1           8  1.497718       1           0.5          1       0   \n",
       "8        1           9  2.240632       0           0.3          1       1   \n",
       "9        1          10  2.611479       0           0.5          1       0   \n",
       "\n",
       "   continuous    dosage  \n",
       "0    2.197827  0.000000  \n",
       "1    4.377510  0.000000  \n",
       "2    2.832585  0.000000  \n",
       "3    2.994290  0.050000  \n",
       "4    4.139551  0.047500  \n",
       "5    2.941154  0.045125  \n",
       "6    2.093228  0.042869  \n",
       "7    4.489415  0.040725  \n",
       "8    2.209942  0.088689  \n",
       "9    3.550368  0.084255  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = 50\n",
    "n = 100\n",
    "\n",
    "# Form Decision Making Policy and Run MRT Study\n",
    "policy = SimplePolicy()\n",
    "study_df = simulate_MRT(T, n, env_params, policy)\n",
    "\n",
    "\n",
    "# Print first 10 rows of dataframe\n",
    "print(\"Average Treatment Prob: {}\".format(np.mean(study_df['action1_prob'])))\n",
    "print(\"\")\n",
    "study_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c7222",
   "metadata": {},
   "source": [
    "## (1) Estimating Causal Excursion Effect \n",
    "$$\n",
    "X_{i,t}^\\top \\theta^\\star = \\mathbb{E} \\left[ Y_{t+1}( \\bar A_{t-1}, 1 ) - Y_{t+1}( \\bar A_{t-1}, 0 ) \\big| X_{i,t} \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\mathrm{argmin_{(\\theta_0, \\theta_1) \\in \\mathbb{R}^2}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^T \\left( Y_{i,t+1} - \\psi(H_{i,t-1}, O_{i,t})^\\top \\eta - (A_{i,t}-\\pi_{i,t}) X_{i,t}^\\top \\theta \\right)^2 \\right\\}\n",
    "$$\n",
    "where $\\psi(H_{i,t-1}, O_{i,t}) = [1, B_{i,t}, C_{i,t}]$ and $X_{i,t} = [1, C_{i,t}]$. Recall that $O_{i,t} = (1, B_{i,t}, C_{i,t})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cdcf375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_feat_matrix(study_df, base_feat_names, treat_feat_names, \n",
    "                     action_centering=False):\n",
    "    \"\"\"\n",
    "    If action_centering = True\n",
    "        Return matrix where each row is [ g(H_{i,t-1}, O_{i,t}), (A_{i,t}-\\pi_{i,t}) X_{i,t} ]\n",
    "    Else:\n",
    "        Return matrix where each row is [ g(H_{i,t-1}, O_{i,t}), A_{i,t} X_{i,t} ]\n",
    "    \"\"\"\n",
    "    base_feats = np.vstack([study_df[feat] for feat in base_feat_names]).T\n",
    "    treat_feats = np.vstack([study_df[feat] for feat in treat_feat_names]).T\n",
    "    actions = actions = study_df['action']\n",
    "    action_probs = study_df['action1_prob']\n",
    "    \n",
    "    if action_centering:\n",
    "        \"\"\"\n",
    "        EXERCISE: Form features with action centering\n",
    "        pass\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        SOLUTION: Form features with action centering\n",
    "        \"\"\"\n",
    "        action_centering = (actions-action_probs).to_numpy()\n",
    "        action_centering = np.expand_dims(action_centering, 1)\n",
    "        feat_matrix = np.concatenate([ base_feats, action_centering*treat_feats], axis=1)\n",
    "    else:\n",
    "        actions = np.expand_dims(actions, 1)\n",
    "        feat_matrix = np.concatenate([ base_feats, actions*treat_feats], axis=1)\n",
    "        \n",
    "    return feat_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a5023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_model(feat_matrix, Y_vec, weights=None):\n",
    "    reg = LinearRegression().fit(feat_matrix, Y_vec, weights)\n",
    "    thetahat = reg.coef_.copy()\n",
    "    thetahat[0] = reg.intercept_\n",
    "    return thetahat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d92f35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True excursion effect (theta):\n",
      "[0.3  0.05]\n",
      "Estimated excursion effect (theta):\n",
      "[0.30317061 0.04688538]\n"
     ]
    }
   ],
   "source": [
    "# Form Least Squares Estimator\n",
    "Y_vec = study_df['reward']\n",
    "feat_matrix = form_feat_matrix(study_df, \n",
    "                               base_feat_names = [\"intercept\", \"binary\", \"continuous\"],\n",
    "                               treat_feat_names = [\"intercept\", \"continuous\"],\n",
    "                               action_centering=True)\n",
    "\n",
    "# Fit Linear Model\n",
    "thetahat = fit_linear_model(feat_matrix, Y_vec)\n",
    "\n",
    "print(\"True excursion effect (theta):\")\n",
    "print( env_params['alpha1'] )\n",
    "print(\"Estimated excursion effect (theta):\")\n",
    "print( thetahat[-2:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4842a57",
   "metadata": {},
   "source": [
    "## (2) Investiagate the Impact of Action Centering\n",
    "$$\n",
    "\\hat{\\theta} = \\mathrm{argmin_{(\\theta_0, \\theta_1) \\in \\mathbb{R}^2}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^T \\left( Y_{i,t+1} - \\psi(H_{i,t-1}, O_{i,t})^\\top \\eta - (A_{i,t}-\\pi_{i,t}) X_{i,t}^\\top \\theta \\right)^2 \\right\\}\n",
    "$$\n",
    "where $\\psi(H_{i,t-1}, O_{i,t}) = [1, B_{i,t}, C_{i,t}]$ and $X_{i,t} = [1, C_{i,t}]$. Recall that $O_{i,t} = (1, B_{i,t}, C_{i,t})$.\n",
    "\n",
    "__Questions:__\n",
    "- What happens if the for the average reward is very bad? For example $\\psi(H_{i,t-1}, O_{i,t}) = 1$?\n",
    "- How does this impact the estimation of the excursion effect theta? Try both with and without action centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddef3046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True excursion effect (theta):\n",
      "[0.3  0.05]\n",
      "Estimated excursion effect - no action centering (theta):\n",
      "[0.29798717 0.0486899 ]\n",
      "Estimated excursion effect - with action centering (theta):\n",
      "[0.30317061 0.04688538]\n"
     ]
    }
   ],
   "source": [
    "# Form Least Squares Estimator no action centering #################\n",
    "\"\"\"\n",
    "EXERCISE: Fit Linear Model without action centering\n",
    "thetahat_noac = np.nan(5)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "SOLUTION: Fit Linear Model without action centering\n",
    "\"\"\"\n",
    "Y_vec = study_df['reward']\n",
    "feat_matrix_noac = form_feat_matrix(study_df, \n",
    "                               base_feat_names = [\"intercept\", \"binary\", \"continuous\"],\n",
    "                               treat_feat_names = [\"intercept\", \"continuous\"],\n",
    "                               action_centering=False)\n",
    "\n",
    "# Fit Linear Model\n",
    "thetahat_noac = fit_linear_model(feat_matrix_noac, Y_vec)\n",
    "\n",
    "# Form Least Squares Estimator with action centering #################\n",
    "\"\"\"\n",
    "# EXERCISE: Fit Linear Model with action centering \n",
    "thetahat_ac = np.zeros(5)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "SOLUTION: Fit Linear Model with action centering\n",
    "\"\"\"\n",
    "Y_vec = study_df['reward']\n",
    "feat_matrix_ac = form_feat_matrix(study_df, \n",
    "                               base_feat_names = [\"intercept\", \"binary\", \"continuous\"],\n",
    "                               treat_feat_names = [\"intercept\", \"continuous\"],\n",
    "                               action_centering=True)\n",
    "\n",
    "# Fit Linear Model\n",
    "thetahat_ac = fit_linear_model(feat_matrix_ac, Y_vec)\n",
    "\n",
    "print(\"True excursion effect (theta):\")\n",
    "print( env_params['alpha1'] )\n",
    "print(\"Estimated excursion effect - no action centering (theta):\")\n",
    "print( thetahat_noac[-2:] )\n",
    "print(\"Estimated excursion effect - with action centering (theta):\")\n",
    "print( thetahat_ac[-2:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a5b19",
   "metadata": {},
   "source": [
    "## (3) Generate MRT Dataset with a Reinforcement Learning Algorithm\n",
    "- Use the following posterior sampling algorithm in a simulated MRT\n",
    "    - The algorithm must only learn using the data of a single user (individual RL algorithms).\n",
    "- Alternatively you can code your own RL Algorithm of choice (e.g. from those Raaz taught earlier)\n",
    "    - Make sure to restrict the action selection probabilities within [$\\pi_\\min, 1-\\pi_\\min$]\n",
    "    - Specifically, fill out the `form_action1_prob` and `update_algorithm` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb4aa1",
   "metadata": {},
   "source": [
    "### Example RL algorithm you could implement: Posterior Sampling\n",
    "- Normal priors and likelihoods\n",
    "- RL algorithm's model of the reward:\n",
    "\n",
    "$$\n",
    "R_{t+1} = \\phi_0(O_t)^\\top \\beta_0 + A_t \\phi_1(O_t)^\\top \\beta_1 + \\epsilon_t\n",
    "$$\n",
    "\n",
    "where $\\phi_0(O_t) = (1, C_t)$, $\\phi_1(O_t) = (1, C_t)$, and $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2_{\\mathrm{alg}})$.\n",
    "\n",
    "- Prior: \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    \\beta_0 \\\\ \n",
    "    \\beta_1\n",
    "\\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, I_4 \\right)\n",
    "$$\n",
    "\n",
    "- Constrain action selection probabilities so for $\\pi_{\\min} = 0.1$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(A_t = 1 | H_{t-1}, O_t) \\in [\\pi_{\\min}, 1 - \\pi_{\\min}] ~~~~ \\mathrm{with~probability}~1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efc4e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosteriorSampler:\n",
    "    \n",
    "    def __init__(self, n, prior_mean, prior_var, sigma_alg, pi_min):\n",
    "        self.sigma_alg = sigma_alg\n",
    "        self.pi_min = pi_min\n",
    "        \n",
    "        all_posteriors = {}\n",
    "        for user_id in range(1, n+1):\n",
    "            posterior = {\n",
    "                \"posterior_mean\": prior_mean.copy(),\n",
    "                \"posterior_var\": prior_var.copy(),\n",
    "            }\n",
    "            all_posteriors[user_id] = posterior\n",
    "            \n",
    "        self.all_posteriors = all_posteriors\n",
    "      \n",
    "    \n",
    "    def form_action1_prob(self, state, user_id):\n",
    "    \n",
    "        # Posterior Mean and Variance for beta_1 (treatment effect)\n",
    "        posterior = self.all_posteriors[user_id]\n",
    "        post_mean_beta1 = posterior['posterior_mean'][-1]\n",
    "        post_var_beta1 = posterior['posterior_var'][-1,-1]\n",
    "    \n",
    "        raw_prob = sp.stats.norm.cdf( post_mean_beta1 / np.sqrt(post_var_beta1) )\n",
    "    \n",
    "        # Restrict action selection probabilities to [pi_min, 1-pi_min]\n",
    "        return self.clip_action1_prob(raw_prob, self.pi_min)\n",
    "    \n",
    "    \n",
    "    def clip_action1_prob(self, raw_prob, pi_min):\n",
    "        clipped_prob = np.maximum(pi_min, raw_prob)\n",
    "        clipped_prob = np.minimum(1-pi_min, clipped_prob)\n",
    "        return clipped_prob\n",
    "        \n",
    "        \n",
    "    def update_algorithm(self, new_states, new_actions, new_rewards, all_user_ids):\n",
    "        \n",
    "        # update posterior for each user\n",
    "        for user_id in all_user_ids:\n",
    "            user_state = new_states[user_id-1]\n",
    "            user_action = new_actions[user_id-1]\n",
    "            user_reward = new_rewards[user_id-1]\n",
    "        \n",
    "            posterior = self.all_posteriors[user_id]\n",
    "            post_var = posterior['posterior_var']\n",
    "            post_mean = posterior['posterior_mean']\n",
    "            feat_vec = np.array([user_state[0], user_state[2], \n",
    "                                 user_action, user_reward*user_state[2]])\n",
    "\n",
    "            inv_post_var = np.linalg.inv(post_var)\n",
    "            new_inv_post_var = inv_post_var + np.outer(feat_vec, feat_vec) / self.sigma_alg\n",
    "            new_post_var = np.linalg.inv( new_inv_post_var )\n",
    "    \n",
    "            new_post_mean_num = np.matmul(inv_post_var, post_mean) + \\\n",
    "                                user_reward * feat_vec / self.sigma_alg\n",
    "            new_post_mean = np.matmul( new_post_var, new_post_mean_num )\n",
    "    \n",
    "            self.all_posteriors[user_id]['posterior_mean'] = new_post_mean\n",
    "            self.all_posteriors[user_id]['posterior_var'] = new_post_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c6cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLAlg:\n",
    "    \n",
    "    def __init__(self, n, pi_min):\n",
    "        self.pi_min = pi_min\n",
    "        self.n\n",
    "        \n",
    "    \n",
    "    def form_action1_prob(self, state, user_id):\n",
    "        \"\"\"\n",
    "        TODO: form action selection probability for a given user and state\n",
    "        \"\"\"\n",
    "        # State variables order: \"intercept\", \"binary\", \"continuous\"\n",
    "        \n",
    "        if state[1] == 1: # binary state variable\n",
    "            raw_prob = 0.3\n",
    "        else:\n",
    "            raw_prob = 0.5\n",
    "    \n",
    "        # Restrict action selection probabilities to [pi_min, 1-pi_min]\n",
    "        return self.clip_action1_prob(raw_prob, self.pi_min)\n",
    "    \n",
    "    \n",
    "    def clip_action1_prob(self, raw_prob, pi_min):\n",
    "        \"\"\"\n",
    "        Constrain action selection probabilities between pi_min and 1-pi_min\n",
    "        \"\"\"\n",
    "        assert 0 < pi_min <= 0.5\n",
    "        clipped_prob = np.maximum(pi_min, raw_prob)\n",
    "        clipped_prob = np.minimum(1-pi_min, clipped_prob)\n",
    "        return clipped_prob\n",
    "        \n",
    "        \n",
    "    def update_algorithm(self, new_states, new_actions, new_rewards, all_user_ids):\n",
    "        \"\"\"\n",
    "        TODO: Update RL algorithm\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ace533",
   "metadata": {},
   "source": [
    "### Generate Dataset with RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f39956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Treatment Prob: 0.9029083692457877\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>decision_t</th>\n",
       "      <th>reward</th>\n",
       "      <th>action</th>\n",
       "      <th>action1_prob</th>\n",
       "      <th>intercept</th>\n",
       "      <th>binary</th>\n",
       "      <th>continuous</th>\n",
       "      <th>dosage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.041791</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.191668</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.524209</td>\n",
       "      <td>0</td>\n",
       "      <td>0.644637</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.232629</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.510612</td>\n",
       "      <td>1</td>\n",
       "      <td>0.654948</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.659192</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.145476</td>\n",
       "      <td>1</td>\n",
       "      <td>0.660858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.087327</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.583447</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915816</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.342691</td>\n",
       "      <td>0.954875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.616093</td>\n",
       "      <td>1</td>\n",
       "      <td>0.921095</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.507763</td>\n",
       "      <td>0.957131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2.190345</td>\n",
       "      <td>1</td>\n",
       "      <td>0.924726</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.597751</td>\n",
       "      <td>0.959275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.028869</td>\n",
       "      <td>1</td>\n",
       "      <td>0.938292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.034398</td>\n",
       "      <td>0.961311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.276377</td>\n",
       "      <td>1</td>\n",
       "      <td>0.946857</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.871314</td>\n",
       "      <td>0.963245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.399894</td>\n",
       "      <td>1</td>\n",
       "      <td>0.947781</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.474531</td>\n",
       "      <td>0.965083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  decision_t    reward  action  action1_prob  intercept  binary  \\\n",
       "0        1           1  2.041791       1      0.500000          1       0   \n",
       "1        1           2  1.524209       0      0.644637          1       0   \n",
       "2        1           3  1.510612       1      0.654948          1       1   \n",
       "3        1           4  0.145476       1      0.660858          1       0   \n",
       "4        1           5  1.583447       1      0.915816          1       1   \n",
       "5        1           6  1.616093       1      0.921095          1       0   \n",
       "6        1           7  2.190345       1      0.924726          1       1   \n",
       "7        1           8  1.028869       1      0.938292          1       0   \n",
       "8        1           9  1.276377       1      0.946857          1       0   \n",
       "9        1          10  1.399894       1      0.947781          1       0   \n",
       "\n",
       "   continuous    dosage  \n",
       "0    3.191668  1.000000  \n",
       "1    3.232629  1.000000  \n",
       "2    2.659192  0.950000  \n",
       "3    3.087327  0.952500  \n",
       "4    1.342691  0.954875  \n",
       "5    1.507763  0.957131  \n",
       "6    3.597751  0.959275  \n",
       "7    3.034398  0.961311  \n",
       "8    2.871314  0.963245  \n",
       "9    2.474531  0.965083  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RL Algorithm Hyperparameters\n",
    "pi_min = 0.05\n",
    "prior_mean = np.array([0.5, 0, 0, 0])\n",
    "prior_var = np.eye(4)\n",
    "sigma_alg = 1\n",
    "\n",
    "\n",
    "# Form RL Algorithm and Run MRT Study\n",
    "RL_alg = PosteriorSampler(n, prior_mean, prior_var, sigma_alg, pi_min)\n",
    "study_df_RL = simulate_MRT(T, n, env_params, RL_alg)\n",
    "\n",
    "\n",
    "# Print first 10 rows of dataframe\n",
    "print(\"Average Treatment Prob: {}\".format(np.mean(study_df_RL['action1_prob'])))\n",
    "print(\"\")\n",
    "study_df_RL.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9852d42d",
   "metadata": {},
   "source": [
    "## (4) Estimating Causal Excursion Effect on RL Data\n",
    "$$\n",
    "X_{i,t}^\\top \\theta^\\star = \\mathbb{E} \\left[ Y_{t+1}( \\bar A_{t-1}, 1 ) - Y_{t+1}( \\bar A_{t-1}, 0 ) | X_{i,t} \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\mathrm{argmin_{(\\theta_0, \\theta_1) \\in \\mathbb{R}^2}} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^T W_{i,t} \\left( Y_{i,t+1} - \\psi(H_{i,t-1}, O_{i,t})^\\top \\eta - (A_{i,t}-p(X_{i,t})) X_{i,t}^\\top \\theta \\right)^2 \\right\\}\n",
    "$$\n",
    "where $\\psi(H_{i,t-1}, O_{i,t}) = [1, B_{i,t}, C_{i,t}]$ and $X_{i,t} = [1, C_{i,t}]$. Recall that $O_{i,t} = (1, B_{i,t}, C_{i,t})$.\n",
    "\n",
    "__Make sure to include weights $W_{i,t}$!!__\n",
    "$$W_{i,t} = \\bigg( \\frac{p(X_{i,t})}{\\pi_{i,t}} \\bigg)^{A_{i,t}} \\bigg( \\frac{1-p(X_{i,t})}{1-\\pi_{i,t}} \\bigg)^{1-A_{i,t}}$$\n",
    "for some pre-specified policy $p$, to ensure that $p(X_{i,t})$ only depends on $X_{i,t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e7f23d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True excursion effect (theta):\n",
      "[0.3  0.05]\n",
      "Estimated excursion effect (theta):\n",
      "[0.4001475  0.03108411]\n"
     ]
    }
   ],
   "source": [
    "# Form Least Squares Estimator\n",
    "Y_vec = study_df['reward']\n",
    "feat_matrix = form_feat_matrix(study_df, \n",
    "                               base_feat_names = [\"intercept\", \"binary\", \"continuous\"],\n",
    "                               treat_feat_names = [\"intercept\", \"continuous\"],\n",
    "                               action_centering=True)\n",
    "\n",
    "\"\"\"\n",
    "EXERCISE: Form Vector of weights\n",
    "action1_probs = study_df_RL['action1_prob'].to_numpy()\n",
    "actions = study_df_RL['action'].to_numpy()\n",
    "weight_vec = np.ones(action1_probs.shape) # EXERCISE: change this to weights W_{i,t}\n",
    "thetahat = fit_linear_model(feat_matrix, Y_vec, weight_vec)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "SOLUTION: Form Vector of weights\n",
    "\"\"\"\n",
    "action1_probs = study_df_RL['action1_prob'].to_numpy()\n",
    "actions = study_df_RL['action'].to_numpy()\n",
    "weight_vec = actions * 0.5 / action1_probs + (1-actions) * 0.5 / (1-action1_probs)\n",
    "thetahat = fit_linear_model(feat_matrix, Y_vec, weight_vec)\n",
    "\n",
    "print(\"True excursion effect (theta):\")\n",
    "print( env_params['alpha1'] )\n",
    "print(\"Estimated excursion effect (theta):\")\n",
    "print( thetahat[-2:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f459e4",
   "metadata": {},
   "source": [
    "## (5) Additional Exploration (on your own time)\n",
    "- What happens if you change $\\pi_\\min$? How do we expect our estimation of the excursion effect to change?\n",
    "- Try estimate a marginal excursion effect (not conditional on state)?\n",
    "- Estimate the limiting variance of the excursion effect and form a confidence interval via a normal approximation. For the variance formula see Proposition 3.1 in [Assessing Time-Varying Causal Effect Moderation in Mobile Health](https://arxiv.org/abs/1601.00237).\n",
    "- You could form a more complex reward generating model. In this setting, one could use a machine learning model (e.g. random forrest) to replace $\\psi(H_{i,t-1}, O_{i,t})^\\top \\eta$. the See [Double/Debiased Machine Learning for Treatment and Causal Parameters](https://arxiv.org/abs/1608.00060)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5b606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
