{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd23872b-22a4-4c9c-8489-41940da36e63",
   "metadata": {},
   "source": [
    "# Contextual bandits and Bayesian Thomposon Sampling Algorithms\n",
    "\n",
    "In this Jupyter Notebook, we will code two variants of BTS with Gaussian linear model \n",
    "\n",
    "1. Bayesian Thompson Sampling\n",
    "2. Bayesian Thompson Sampling V2 (with exact posterior probability computation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb094f-31c5-4bca-82a6-871a8863dc35",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports\n",
    "\n",
    "Let's start by importing the required libraries and setting up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58018f9-663f-40e0-bdaf-a4e9a82a75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "# Define utility functions\n",
    "def pplot(ax=None):\n",
    "    if ax is None:\n",
    "        plt.grid(True, alpha=0.5)\n",
    "        axoff(plt.gca())\n",
    "    else:\n",
    "        ax.grid(True, alpha=0.5)\n",
    "        axoff(ax)\n",
    "    return\n",
    "\n",
    "def axoff(ax, keys=['top', 'right']):\n",
    "    for k in keys:\n",
    "        ax.spines[k].set_visible(False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05431cdc-10a9-4bf9-a0d2-f954d0f58aac",
   "metadata": {},
   "source": [
    "## Bandit Environment and Algorithm Classes\n",
    "\n",
    "Next, let's define the BanditEnvironment class and the classes for different bandit algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d791d7b-bdea-490d-8d6f-14b9196c292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualBanditEnvironment:\n",
    "    def __init__(self, num_arms, num_features, true_params):\n",
    "        self.num_arms = num_arms\n",
    "        self.num_features = num_features\n",
    "        self.true_params = true_params # True parameters for each arm\n",
    "    \n",
    "    def pull_arm(self, arm, state):\n",
    "        reward = np.dot(self.true_params[arm], state) + NOISE_STD * np.random.randn()\n",
    "        return(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d564c-3a39-45d5-81e2-652d849ea791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianLinearModel:\n",
    "    def __init__(self, num_features, weights, covariance, noise_variance):\n",
    "        self.num_features = num_features\n",
    "        self.mean_params = weights  \n",
    "        self.covariance = covariance \n",
    "        self.noise_variance = noise_variance \n",
    "    \n",
    "    def sample_weights(self):\n",
    "        ''' This generates weights using the mean and the covariance'''\n",
    "        return(np.random.multivariate_normal(self.mean_params, self.covariance))\n",
    "    \n",
    "    def update(self, reward, state):\n",
    "        precision = np.linalg.inv(self.covariance)\n",
    "        posterior_precision = precision + np.outer(state, state) / NOISE_STD**2\n",
    "        posterior_covariance = np.linalg.inv(posterior_precision)\n",
    "        posterior_mean = np.dot(posterior_covariance, np.dot(precision, self.mean_params) + reward * state / self.noise_variance)\n",
    "        self.mean_params = posterior_mean\n",
    "        self.covariance = posterior_covariance\n",
    "\n",
    "\n",
    "class BayesianThompsonSampling:\n",
    "    def __init__(self, num_arms, num_features, prior_means, prior_variances, arm_variances):\n",
    "        self.num_arms = num_arms\n",
    "        self.num_features = num_features\n",
    "        self.arm_models = [GaussianLinearModel(num_features, prior_means[arm], prior_variances[arm], arm_variances[arm]) for arm in range(num_arms)]\n",
    "        self.arm_variances = arm_variances\n",
    "    \n",
    "    def select_arm(self, state):\n",
    "        arm_samples = [self.arm_models[arm].sample_weights() for arm in range(num_arms)]\n",
    "        arm_rewards = [np.dot(arm_samples[arm], state) for arm in range(num_arms)]\n",
    "        return(np.argmax(arm_rewards))\n",
    "\n",
    "    def update(self, arm, reward, state):\n",
    "        self.arm_models[arm].update(reward, state)\n",
    "\n",
    "\n",
    "class BayesianThompsonSamplingV2:\n",
    "    def __init__(self, num_arms, num_features, prior_means, prior_variances, arm_variances):\n",
    "        self.num_arms = num_arms\n",
    "        self.num_features = num_features\n",
    "        self.arm_models = [GaussianLinearModel(num_features, prior_means[arm], prior_variances[arm], arm_variances[arm]) for arm in range(num_arms)]\n",
    "        self.arm_variances = arm_variances\n",
    "    \n",
    "    def select_arm(self, state):\n",
    "        mean_diff = np.dot(self.arm_models[1].mean_params - self.arm_models[0].mean_params, state)\n",
    "        var_diff = np.dot( (self.arm_models[1].covariance +  self.arm_models[0].covariance) @ state, state) \n",
    "        p0 = norm.cdf(x=0, loc=mean_diff, scale=np.sqrt(var_diff)) # probability that P(mu1-mu0<0)\n",
    "        return 1-p0, np.random.choice(self.num_arms, p=[p0, 1-p0])\n",
    "\n",
    "    def update(self, arm, reward, state):\n",
    "        self.arm_models[arm].update(reward, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6a3b4-fceb-4ca9-9582-5696d9e9b18e",
   "metadata": {},
   "source": [
    "#### %%%%%%% YOU WILL EDIT THIS CELL AT SOME POINT %%%%%%% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66759ace-bac9-4e27-9944-590da18af63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianThompsonSampling:\n",
    "    def __init__(self, num_arms, num_features, prior_means, prior_variances, arm_variances):\n",
    "        self.num_arms = num_arms\n",
    "        self.num_features = num_features\n",
    "        self.arm_models = [GaussianLinearModel(num_features, prior_means[arm], prior_variances[arm], arm_variances[arm]) for arm in range(num_arms)]\n",
    "        self.arm_variances = arm_variances\n",
    "    \n",
    "    def select_arm(self, state):\n",
    "        arm_samples = [self.arm_models[arm].sample_weights() for arm in range(num_arms)]\n",
    "        arm_rewards = [np.dot(arm_samples[arm], state) for arm in range(num_arms)]\n",
    "        return(np.argmax(arm_rewards))\n",
    "\n",
    "    def update(self, arm, reward, state):\n",
    "        self.arm_models[arm].update(reward, state)\n",
    "\n",
    "\n",
    "class BayesianThompsonSamplingV2:\n",
    "    def __init__(self, num_arms, num_features, prior_means, prior_variances, arm_variances):\n",
    "        self.num_arms = num_arms\n",
    "        self.num_features = num_features\n",
    "        self.arm_models = [GaussianLinearModel(num_features, prior_means[arm], prior_variances[arm], arm_variances[arm]) for arm in range(num_arms)]\n",
    "        self.arm_variances = arm_variances\n",
    "    \n",
    "    def select_arm(self, state):\n",
    "        mean_diff = np.dot(self.arm_models[1].mean_params - self.arm_models[0].mean_params, state)\n",
    "        var_diff = np.dot( (self.arm_models[1].covariance +  self.arm_models[0].covariance) @ state, state) \n",
    "        p0 = norm.cdf(x=0, loc=mean_diff, scale=np.sqrt(var_diff)) # probability that P(mu1-mu0<0)\n",
    "        return 1-p0, np.random.choice(self.num_arms, p=[p0, 1-p0])\n",
    "\n",
    "    def update(self, arm, reward, state):\n",
    "        self.arm_models[arm].update(reward, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff66b0-d6b7-4cd5-8861-a7242a11fcb1",
   "metadata": {},
   "source": [
    "## Algorithm Comparison\n",
    "\n",
    "Now, let's compare the performance of different algorithms in the bandit environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa76717-006a-4d7c-8706-a0a5e8e0e756",
   "metadata": {},
   "source": [
    "#### %%%%%%% YOU WILL EDIT THIS CELL AT SOME POINT %%%%%%% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f213071-acba-4103-af72-c0e88124eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%%%%%% YOU WILL EDIT CODE HERE %%%%%%% \n",
    "# np.random.seed(1000)\n",
    "np.random.seed(100)\n",
    "EVAL_BTS = True # CHANGE THIS TO True when you finish EXERCISE 1 / Editing BTS code.\n",
    "EVAL_BTSV2 = True # CHANGE THIS TO True when you finish EXERCISE 2 / Editing BTSV2 code.\n",
    "## %%%%%%% UNTIL HERE %%%%%%% \n",
    "\n",
    "\n",
    "NUM_ARMS  = 2\n",
    "NUM_FEATURES  = 2\n",
    "TRUE_PARAMS = np.array(([1., 0.5], [0.5, 1])) #[1, 0.5] is arm 0's parameter\n",
    "NOISE_STD = 1. # (Gaussian) noise std in rewards\n",
    "NOISE_STD_EST = 1.\n",
    "\n",
    "assert(TRUE_PARAMS.shape[0]==NUM_ARMS)\n",
    "assert(TRUE_PARAMS.shape[1]==NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece5ba6-fed3-44a8-b119-c04088a92bb3",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11cec2-4c8c-4965-9725-9202a5fce55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_arms = NUM_ARMS\n",
    "num_features = NUM_FEATURES\n",
    "num_rounds = 1000\n",
    "prior_means = np.zeros((num_arms, num_features))\n",
    "prior_variances = np.array((np.eye(num_features), np.eye(num_features)))\n",
    "arm_variances = NOISE_STD_EST**2 * np.ones(num_arms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42457cef-8cd4-4af4-a991-c774fe8a34e0",
   "metadata": {},
   "source": [
    "### Create Bandit environment and algs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d01716-b159-4dcb-9e3b-4521f9d1250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bandit_env = ContextualBanditEnvironment(num_arms, num_features, TRUE_PARAMS)\n",
    "true_params = bandit_env.true_params\n",
    "\n",
    "\n",
    "algs = []\n",
    "if EVAL_BTS:\n",
    "    bts = BayesianThompsonSampling(num_arms, num_features, prior_means, prior_variances, arm_variances)\n",
    "    algs.append(('Bayes. Thomp. Sampling', bts))\n",
    "if EVAL_BTSV2:\n",
    "    btsv2 = BayesianThompsonSamplingV2(num_arms, num_features, prior_means, prior_variances, arm_variances)\n",
    "    algs.append(('Bayes. Thomp. Sampling V2', btsv2))\n",
    "    p_bts2 = np.zeros(num_rounds) # probabilities of selecting arm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0cbe0-65dc-4d74-83e7-3c20d0f75b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store results for each alg\n",
    "results = {alg: {\n",
    "    'chosen_arms': np.zeros(num_rounds, dtype=int),\n",
    "    'rewards': np.zeros(num_rounds),\n",
    "    'cumulative_rewards': np.zeros(num_rounds),\n",
    "    'cumulative_regret': np.zeros(num_rounds),\n",
    "    'cumulative_pulls': np.zeros(num_rounds)\n",
    "} for alg, _ in algs}\n",
    "\n",
    "states = np.zeros((num_rounds,num_features))\n",
    "true_rewards = np.vstack([states @ true_params[0], states @ true_params[1]]).T\n",
    "optimal_arms = np.zeros(num_rounds, dtype=int)\n",
    "for t in range(num_rounds):\n",
    "    for alg, alg_obj in algs:\n",
    "        # Generate a random state/context\n",
    "        states[t] = np.random.randn(num_features)\n",
    "        true_rewards[t] = true_params @ states[t]\n",
    "        optimal_arms[t] = np.argmax(true_rewards[t])\n",
    "\n",
    "        # Select arm using the alg\n",
    "        if alg == \"Bayes. Thomp. Sampling V2\":\n",
    "            p_bts2[t], chosen_arm = alg_obj.select_arm(states[t])\n",
    "        else:\n",
    "            chosen_arm = alg_obj.select_arm(states[t])\n",
    "\n",
    "        results[alg]['chosen_arms'][t] = chosen_arm\n",
    "        \n",
    "        # Pull arm and get reward\n",
    "        reward = bandit_env.pull_arm(chosen_arm, states[t])\n",
    "        results[alg]['rewards'][t] = reward\n",
    "\n",
    "        # Update arm model\n",
    "        alg_obj.update(chosen_arm, reward, states[t])\n",
    "\n",
    "        # Calculate cumulative rewards using the relation\n",
    "        results[alg]['cumulative_rewards'][t] = results[alg]['cumulative_rewards'][t-1] + reward if t > 0 else reward\n",
    "        \n",
    "        # Calculate cumulative regret using the relation\n",
    "        regret = np.max(true_rewards[t]) - true_rewards[t, chosen_arm]\n",
    "        results[alg]['cumulative_regret'][t] = results[alg]['cumulative_regret'][t-1] + regret  if t > 0 else regret\n",
    "        \n",
    "        # Calculate cumulative pulls\n",
    "        results[alg]['cumulative_pulls'][t] = (results[alg]['cumulative_pulls'][t-1]*t + (chosen_arm == optimal_arms[t]))/(t+1) if t>0 else chosen_arm == optimal_arms[t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925f7ea-4f8c-4399-b92b-67c3387f45e2",
   "metadata": {},
   "source": [
    "# Plot cumulative rewards, cumulative regret, and number of pulls for each algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1221e-c4c5-41c8-98fc-de9e3ab876e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for alg, data in results.items():\n",
    "    axs[0].plot(range(num_rounds), data['cumulative_rewards'], label=alg)\n",
    "    axs[1].plot(range(num_rounds), data['cumulative_regret'], label=alg)\n",
    "    axs[2].plot(range(num_rounds), data['cumulative_pulls'], label=alg)\n",
    "\n",
    "axs[0].set_ylabel(\"Cumulative Rewards\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_ylabel(\"Cumulative Regret\")\n",
    "\n",
    "axs[2].set_ylabel(\"Cumulative Number of Optimal Arm Pulls/Time Step\")\n",
    "\n",
    "for j in range(3):\n",
    "    axs[j].set_xlabel(\"Time Step\")\n",
    "    pplot(axs[j])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e33990-05f5-4b5b-926a-549cbce8b91b",
   "metadata": {},
   "source": [
    "\n",
    "## Questions\n",
    "\n",
    "1. What would happen to the regrets of different algorithms if we:\n",
    "   - Reduce the separation between the two arms?\n",
    "   - Increase noise variance in rewards?\n",
    "   - Noise variance estimate was wrong?\n",
    "   - Increase the number of rounds?\n",
    "2. BONUS: Try to empirically check the scaling rate of regret with the number of decision times using a Log-scale and least squares fit.\n",
    "3. Play around with random seeds and other problem parameters. Try something fun and creative. If you find something interesting, share it with us!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
