{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd23872b-22a4-4c9c-8489-41940da36e63",
   "metadata": {},
   "source": [
    "# (Simplified) Bayesian Thompson Sampling for Tabular MDP\n",
    "\n",
    "In this Jupyter Notebook, we will code four variants of BTS with Gaussian linear model for MDP\n",
    "\n",
    "1. Bayesian Thompson Sampling that ignores any delayed effects\n",
    "2. Bayesian Thompson Sampling V2 (with exact posterior probability computation) that ignored delayed effects\n",
    "3. Bayesian Thompson Sampling H that incorporates delayed effects (assumes perfect knowledge of delayed effects H)\n",
    "3. Bayesian Thompson Sampling H V2 (with exact posterior probability computation) that incorporates delayed effects (assumes perfect knowledge of delayed effects H)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb094f-31c5-4bca-82a6-871a8863dc35",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports\n",
    "\n",
    "Let's start by importing the required libraries and setting up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58018f9-663f-40e0-bdaf-a4e9a82a75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "# Define utility functions\n",
    "def pplot(ax=None):\n",
    "    if ax is None:\n",
    "        plt.grid(True, alpha=0.5)\n",
    "        axoff(plt.gca())\n",
    "    else:\n",
    "        ax.grid(True, alpha=0.5)\n",
    "        axoff(ax)\n",
    "    return\n",
    "\n",
    "def axoff(ax, keys=['top', 'right']):\n",
    "    for k in keys:\n",
    "        ax.spines[k].set_visible(False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05431cdc-10a9-4bf9-a0d2-f954d0f58aac",
   "metadata": {},
   "source": [
    "## Tabular MDP Environment and Thompson Sampling Algorithm Classes\n",
    "\n",
    "Next, let's define the MDP class and the classes for different variants of BTS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d791d7b-bdea-490d-8d6f-14b9196c292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPEnvironment:\n",
    "    def __init__(self, num_actions, num_states, true_rewards, transition_matrix):\n",
    "        self.num_actions = num_actions\n",
    "        self.num_states = num_states\n",
    "        self.true_rewards = true_rewards\n",
    "        self.transition_matrix = transition_matrix\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        next_state = np.random.choice(self.num_states, p=self.transition_matrix[state, :, action])\n",
    "        reward = self.true_rewards[next_state] + np.random.randn()\n",
    "        return next_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6a3b4-fceb-4ca9-9582-5696d9e9b18e",
   "metadata": {},
   "source": [
    "#### %%%%%%% YOU WILL EDIT THIS CELL AT SOME POINT %%%%%%% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66759ace-bac9-4e27-9944-590da18af63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianThompsonSampling:\n",
    "    def __init__(self, num_actions, num_states, prior_means, prior_variances, reward_variance):\n",
    "        self.num_actions = num_actions\n",
    "        self.prior_means = prior_means\n",
    "        self.prior_variances = prior_variances\n",
    "        self.reward_variance = reward_variance\n",
    "    \n",
    "    ## %%%%%%% EXERCISE 1-----YOU WILL WRITE CODE HERE %%%%%%% \n",
    "    def select_action(self, state):\n",
    "        ''' IMPLEMENT THIS CODE to select arm in Thompson sampling. \n",
    "        This part uses ONLY rewards and NO KNOWELDGE OF HStar / DELAYED EFFECTS.\n",
    "        Your code should involve generating some random variables directly and taking some maxima.\n",
    "        Your code should return the sampled arm.'''\n",
    "        action_samples = np.random.normal(self.prior_means[state], np.sqrt(self.prior_variances[state]))\n",
    "        return np.argmax(action_samples)\n",
    "    ## %%%%%%% UNTIL HERE %%%%%%% \n",
    "\n",
    "    def update(self, state, action, reward):\n",
    "        posterior_precision = 1.0 / self.prior_variances[state, action] + 1.0 / self.reward_variance\n",
    "        posterior_mean = (self.prior_means[state, action] / self.prior_variances[state, action] + reward / self.reward_variance) / posterior_precision\n",
    "        self.prior_means[state, action] = posterior_mean\n",
    "        self.prior_variances[state, action] = 1.0 / posterior_precision\n",
    "\n",
    "\n",
    "\n",
    "class BayesianThompsonSamplingH:\n",
    "    def __init__(self, num_actions, num_states, prior_means, prior_variances, reward_variance):\n",
    "        self.num_actions = num_actions\n",
    "        self.prior_means = prior_means\n",
    "        self.prior_variances = prior_variances\n",
    "        self.reward_variance = reward_variance\n",
    "    \n",
    "    ## %%%%%%% EXERCISE 2-----YOU WILL WRITE CODE HERE %%%%%%% \n",
    "    def select_action(self, state):\n",
    "        ''' IMPLEMENT THIS CODE to select arm in Thompson sampling. \n",
    "        This part uses BOTH rewards and PERFECT KNOWELDGE OF HStar / DELAYED EFFECTS.'''\n",
    "        action_samples = np.random.normal(self.prior_means[state]+HSTAR[state], np.sqrt(self.prior_variances[state]))\n",
    "        return np.argmax(action_samples)\n",
    "    ## %%%%%%% UNTIL HERE %%%%%%% \n",
    "\n",
    "    def update(self, state, action, reward):\n",
    "        posterior_precision = 1.0 / self.prior_variances[state, action] + 1.0 / self.reward_variance\n",
    "        posterior_mean = (self.prior_means[state, action] / self.prior_variances[state, action] + reward / self.reward_variance) / posterior_precision\n",
    "        self.prior_means[state, action] = posterior_mean\n",
    "        self.prior_variances[state, action] = 1.0 / posterior_precision\n",
    "\n",
    "\n",
    "class BayesianThompsonSamplingV2:\n",
    "    def __init__(self, num_actions, num_states, prior_means, prior_variances, reward_variance):\n",
    "        self.num_actions = num_actions\n",
    "        self.prior_means = prior_means\n",
    "        self.prior_variances = prior_variances\n",
    "        self.reward_variance = reward_variance\n",
    "    \n",
    "    ## %%%%%%% EXERCISE 3----- %%%%%%% \n",
    "    def select_action(self, state):\n",
    "        '''IMPLEMENT THIS CODE. Your code should involve computing some probability and then generating\n",
    "       the arm directly. Your code should return the probability of sampling arm 1, and the sampled arm.\n",
    "        his part uses ONLY rewards and NO KNOWELDGE OF HStar / DELAYED EFFECTS.'''\n",
    "        mean_diff = self.prior_means[state, 1] - self.prior_means[state, 0]\n",
    "        var_diff = self.prior_variances[state, 1] + self.prior_variances[state, 0]\n",
    "        p0 = norm.cdf(x=0, loc=mean_diff, scale=np.sqrt(var_diff)) # probability that P(mu1-mu0<0)\n",
    "        return 1-p0, np.random.choice(self.num_actions, p=[p0, 1-p0])\n",
    "    ## %%%%%%% UNTIL HERE %%%%%%% \n",
    "\n",
    "    def update(self, state, action, reward):\n",
    "        posterior_precision = 1.0 / self.prior_variances[state, action] + 1.0 / self.reward_variance\n",
    "        posterior_mean = (self.prior_means[state, action] / self.prior_variances[state, action] + reward / self.reward_variance) / posterior_precision\n",
    "        self.prior_means[state, action] = posterior_mean\n",
    "        self.prior_variances[state, action] = 1.0 / posterior_precision\n",
    "\n",
    "\n",
    "class BayesianThompsonSamplingHV2:\n",
    "    def __init__(self, num_actions, num_states, prior_means, prior_variances, reward_variance):\n",
    "        self.num_actions = num_actions\n",
    "        self.prior_means = prior_means\n",
    "        self.prior_variances = prior_variances\n",
    "        self.reward_variance = reward_variance\n",
    "    \n",
    "    ## %%%%%%% EXERCISE 4-----%%%%%%% \n",
    "    def select_action(self, state):\n",
    "        '''IMPLEMENT THIS CODE. Your code should involve computing some probability and then generating\n",
    "       the arm directly. Your code should return the probability of sampling arm 1, and the sampled arm.\n",
    "        his part uses BOTH rewards and PERFECT KNOWELDGE OF HStar / DELAYED EFFECTS.'''\n",
    "        mean_diff = self.prior_means[state, 1]+ HSTAR[state, 1] - self.prior_means[state, 0] - HSTAR[state, 0]\n",
    "        var_diff = self.prior_variances[state, 1] + self.prior_variances[state, 0]\n",
    "        p0 = norm.cdf(x=0, loc=mean_diff, scale=np.sqrt(var_diff)) # probability that P(mu1-mu0<0)\n",
    "        return 1-p0, np.random.choice(self.num_actions, p=[p0, 1-p0])\n",
    "    ## %%%%%%% UNTIL HERE %%%%%%% \n",
    "\n",
    "    def update(self, state, action, reward):\n",
    "        posterior_precision = 1.0 / self.prior_variances[state, action] + 1.0 / self.reward_variance\n",
    "        posterior_mean = (self.prior_means[state, action] / self.prior_variances[state, action] + reward / self.reward_variance) / posterior_precision\n",
    "        self.prior_means[state, action] = posterior_mean\n",
    "        self.prior_variances[state, action] = 1.0 / posterior_precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff66b0-d6b7-4cd5-8861-a7242a11fcb1",
   "metadata": {},
   "source": [
    "## Algorithm Comparison\n",
    "\n",
    "Now, let's compare the performance of different algorithms in the bandit environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa76717-006a-4d7c-8706-a0a5e8e0e756",
   "metadata": {},
   "source": [
    "#### %%%%%%% YOU WILL EDIT THIS CELL AT SOME POINT %%%%%%% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f213071-acba-4103-af72-c0e88124eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) ### TRY 0, 10, 5\n",
    "\n",
    "EVAL_BTS = True # CHANGE THIS TO True when you finish EXERCISE 1 / Editing BTS code.\n",
    "EVAL_BTSH = True # CHANGE THIS TO True when you finish EXERCISE 2 / Editing BTSH code.\n",
    "EVAL_BTSV2 = True # CHANGE THIS TO True when you finish EXERCISE 3 / Editing BTSV2 code.\n",
    "EVAL_BTSHV2 = True # CHANGE THIS TO True when you finish EXERCISE 4 / Editing BTSHV2 code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece5ba6-fed3-44a8-b119-c04088a92bb3",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11cec2-4c8c-4965-9725-9202a5fce55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 2\n",
    "NUM_STATES = 3\n",
    "NOISE_STD = 3.  # (Gaussian) noise std in rewards\n",
    "NOISE_STD_EST = 3.\n",
    "TRUE_REWARDS = np.array([0, 1, 10])\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "delta = 0.2\n",
    "\n",
    "TRANSITION_MATRIX = np.empty([NUM_STATES, NUM_STATES, NUM_ACTIONS])\n",
    "TRANSITION_MATRIX[0, :, 0] = np.array([1-delta, 0, delta])\n",
    "TRANSITION_MATRIX[1, :, 0] = np.array([1, 0, 0])\n",
    "TRANSITION_MATRIX[2, :, 0] = np.array([1, 0, 0])\n",
    "TRANSITION_MATRIX[0, :, 1] = np.array([0, 1, 0])\n",
    "TRANSITION_MATRIX[1, :, 1] = np.array([0, 1, 0])\n",
    "TRANSITION_MATRIX[2, :, 1] = np.array([0, 0, 1])\n",
    "\n",
    "TRUE_REWARD_MATRIX = np.array([[10*delta, 1], [0, 1], [0, 10]])\n",
    "ALPHA = 10*delta * (1+gamma) / (1-gamma*(1-delta))\n",
    "VSTAR = [166.66666667, 158.33333333, 200.]\n",
    "HSTAR = np.empty([NUM_STATES, NUM_ACTIONS])\n",
    "HSTAR[0, :] = np.array([(1-delta)*VSTAR[0] + delta*VSTAR[2], VSTAR[1]])\n",
    "HSTAR[1, :] = np.array([VSTAR[0], VSTAR[1]])\n",
    "HSTAR[2, :] = [VSTAR[0], VSTAR[2]]\n",
    "\n",
    "\n",
    "num_actions = NUM_ACTIONS\n",
    "num_states = NUM_STATES\n",
    "true_rewards = TRUE_REWARDS\n",
    "transition_matrix = TRANSITION_MATRIX\n",
    "\n",
    "num_rounds = 50\n",
    "prior_means = np.zeros((num_states, num_actions))\n",
    "prior_variances = np.ones((num_states, num_actions))\n",
    "reward_variance = NOISE_STD_EST**2  #* np.ones(num_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42457cef-8cd4-4af4-a991-c774fe8a34e0",
   "metadata": {},
   "source": [
    "### Create MDP environment and algs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d01716-b159-4dcb-9e3b-4521f9d1250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdp_env = MDPEnvironment(num_actions=num_actions, num_states=num_states, true_rewards=true_rewards, \n",
    "                         transition_matrix=transition_matrix)\n",
    "\n",
    "\n",
    "algs = []\n",
    "if EVAL_BTS:\n",
    "    bts = BayesianThompsonSampling(num_actions, num_states, prior_means, prior_variances, reward_variance)\n",
    "    algs.append(('Bayesian Thompson Sampling', bts))\n",
    "if EVAL_BTSV2:\n",
    "    bts2 = BayesianThompsonSamplingV2(num_actions, num_states, prior_means, prior_variances, reward_variance)\n",
    "    algs.append(('Bayesian Thompson Sampling V2', bts2))\n",
    "    p_bts2 = np.zeros(num_rounds)  # posterior probs\n",
    "if EVAL_BTSH:\n",
    "    btsH = BayesianThompsonSamplingH(num_actions, num_states, prior_means, prior_variances, reward_variance)\n",
    "    algs.append(('Bayesian Thompson Sampling with HStar', btsH))\n",
    "if EVAL_BTSHV2:\n",
    "    btsH2 = BayesianThompsonSamplingHV2(num_actions, num_states, prior_means, prior_variances, reward_variance)\n",
    "    algs.append(('Bayesian Thompson Sampling with HStar V2', btsH2))\n",
    "    p_btsH2 = np.zeros(num_rounds)  # posterior probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446aae36-88c0-4559-9062-9feff051b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of algorithms\n",
    "algs = []\n",
    "if EVAL_BTS:\n",
    "    bts = BayesianThompsonSampling(num_actions, num_states, prior_means, prior_variances, reward_variance)\n",
    "    algs.append(('Bayesian Thompson Sampling', bts))\n",
    "if EVAL_BTSV2:\n",
    "    bts2 = BayesianThompsonSamplingV2(num_actions, num_states, prior_means, prior_variances, reward_variance)\n",
    "    algs.append(('Bayesian Thompson Sampling V2', bts2))\n",
    "    p_bts2 = np.zeros(num_rounds)  # posterior probs\n",
    "if EVAL_BTSH:\n",
    "    btsH = BayesianThompsonSamplingH(num_actions, num_states, prior_means, prior_variances, reward_variance)\n",
    "    algs.append(('Bayesian Thompson Sampling with HStar', btsH))\n",
    "if EVAL_BTSHV2:\n",
    "    btsH2 = BayesianThompsonSamplingHV2(num_actions, num_states, prior_means, prior_variances, reward_variance)\n",
    "    algs.append(('Bayesian Thompson Sampling with HStar V2', btsH2))\n",
    "    p_btsH2 = np.zeros(num_rounds)  # posterior probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af0cbe0-65dc-4d74-83e7-3c20d0f75b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store results for each algorithm\n",
    "results = {alg: {\n",
    "    'chosen_actions': np.zeros(num_rounds, dtype=int),\n",
    "    'rewards': np.zeros(num_rounds),\n",
    "    'cumulative_rewards': np.zeros(num_rounds),\n",
    "    'cumulative_pulls': np.zeros(num_rounds),\n",
    "    'states': np.zeros(num_rounds+1, dtype=int)  # 2D array to store states for each algorithm\n",
    "} for alg, _ in algs}\n",
    "\n",
    "true_rewards = np.zeros((num_rounds, num_actions), dtype=int)\n",
    "optimal_actions = np.array([0, 0, 1], dtype=int)\n",
    "\n",
    "state0 = 0\n",
    "for alg, alg_obj in algs:\n",
    "    results[alg]['states'][0] = state0\n",
    "\n",
    "for t in range(num_rounds):\n",
    "    for alg, alg_obj in algs:\n",
    "        # Select action using the algorithm\n",
    "        if alg == 'Bayesian Thompson Sampling':\n",
    "            chosen_action = alg_obj.select_action(results[alg]['states'][t])\n",
    "        elif alg == 'Bayesian Thompson Sampling V2':\n",
    "            p_bts2[t], chosen_action = alg_obj.select_action(results[alg]['states'][t])\n",
    "        elif alg == 'Bayesian Thompson Sampling with HStar':\n",
    "            chosen_action = alg_obj.select_action(results[alg]['states'][t])\n",
    "        else:  # alg == 'Bayesian Thompson Sampling with HStar V2'\n",
    "            p_btsH2[t], chosen_action = alg_obj.select_action(results[alg]['states'][t])\n",
    "\n",
    "        # Transition to the next state and receive the reward\n",
    "        results[alg]['states'][t + 1], reward = mdp_env.transition(results[alg]['states'][t], chosen_action)\n",
    "\n",
    "        # print(results[alg]['states'][t], chosen_action, reward)\n",
    "        # Update the algorithm with the new observation\n",
    "        alg_obj.update(results[alg]['states'][t], chosen_action, reward)\n",
    "\n",
    "        # Store results in the dictionary\n",
    "        results[alg]['chosen_actions'][t] = chosen_action\n",
    "        results[alg]['rewards'][t] = reward\n",
    "\n",
    "        # Calculate cumulative rewards using the relation\n",
    "        results[alg]['cumulative_rewards'][t] = results[alg]['cumulative_rewards'][t-1] + reward if t > 0 else reward\n",
    "\n",
    "        # Calculate cumulative pulls\n",
    "        results[alg]['cumulative_pulls'][t] = (results[alg]['cumulative_pulls'][t-1] * t + (chosen_action == optimal_actions[results[alg]['states'][t]])) / (t+1) if t > 0 else chosen_action == optimal_actions[results[alg]['states'][t]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925f7ea-4f8c-4399-b92b-67c3387f45e2",
   "metadata": {},
   "source": [
    "# Plot cumulative rewards, cumulative regret, and number of pulls for each algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1221e-c4c5-41c8-98fc-de9e3ab876e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for alg, data in results.items():\n",
    "\n",
    "    axs[0].plot(range(num_rounds), data['cumulative_rewards'], label=alg)\n",
    "    axs[1].plot(range(num_rounds), data['cumulative_pulls'], label=alg)\n",
    "\n",
    "    # Plot the state feature over time\n",
    "    axs[2].plot(range(num_rounds + 1), data['states'], label=alg)\n",
    "\n",
    "\n",
    "axs[0].set_ylabel(\"Cumulative Rewards\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_ylabel(\"Fraction of Optimal Arm Pulls\")\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].set_ylabel(\"State \")\n",
    "axs[2].legend()\n",
    "\n",
    "for j in range(3):\n",
    "    axs[j].set_xlabel(\"Time Step\")\n",
    "    pplot(axs[j])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e33990-05f5-4b5b-926a-549cbce8b91b",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "You can explore different scenarios and answer the following questions:\n",
    "\n",
    "1. What would happen to the performance of different algorithms if:\n",
    "   - We reduce the separation between the multiple arms? (Change it in the TRUE_REWARDS variable)\n",
    "   - We increase the noise variance in rewards? (Change it in the NOISE_STD variable)\n",
    "   - Noise variance estimate was wrong? (Change it in the NOISE_STD_EST variable)\n",
    "   - We increase the number of rounds? (Change it in the num_rounds variable)\n",
    "\n",
    "2. Bonus:\n",
    "   - Play around with random seeds, other problem parameters like true_rewards, and delta.\n",
    "   - Try to predict the impact of your changes and then verify empirically.\n",
    "   - If you find something interesting, share it with us!\n",
    "   - Check out the code for value iteration that computes Vstar and Hstar and play with it.\n",
    "\n",
    "Remember to re-run the entire notebook after making changes to see the updated results. Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45eb637-51d5-488c-a990-b253b2d430ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
