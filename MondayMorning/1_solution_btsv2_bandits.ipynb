{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd23872b-22a4-4c9c-8489-41940da36e63",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Algorithms Comparison\n",
    "\n",
    "In this Jupyter Notebook, we will compare different multi-armed bandit algorithms on a two-armed bandit problem. The following algorithms will be used:\n",
    "\n",
    "1. Bayesian Thompson Sampling\n",
    "2. UCB (Upper Confidence Bound)\n",
    "3. Epsilon-Greedy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb094f-31c5-4bca-82a6-871a8863dc35",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports\n",
    "\n",
    "Let's start by importing the required libraries and setting up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58018f9-663f-40e0-bdaf-a4e9a82a75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "# Define utility functions\n",
    "def pplot(ax=None):\n",
    "    if ax is None:\n",
    "        plt.grid(True, alpha=0.5)\n",
    "        axoff(plt.gca())\n",
    "    else:\n",
    "        ax.grid(True, alpha=0.5)\n",
    "        axoff(ax)\n",
    "    return\n",
    "\n",
    "def axoff(ax, keys=['top', 'right']):\n",
    "    for k in keys:\n",
    "        ax.spines[k].set_visible(False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05431cdc-10a9-4bf9-a0d2-f954d0f58aac",
   "metadata": {},
   "source": [
    "## Bandit Environment and Algorithm Classes\n",
    "\n",
    "Next, let's define the BanditEnvironment class and the classes for different bandit algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d791d7b-bdea-490d-8d6f-14b9196c292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditEnvironment:\n",
    "    def __init__(self, num_arms, true_rewards):\n",
    "        self.num_arms = num_arms\n",
    "        self.true_rewards = true_rewards\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        reward = self.true_rewards[arm] + NOISE_STD * np.random.randn()\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f9c82-92e7-4662-8777-a1dcc478f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UCB:\n",
    "    def __init__(self, num_arms):\n",
    "        self.num_arms = num_arms\n",
    "        self.arm_counts = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.time_step = 0\n",
    "    \n",
    "    def select_arm(self):\n",
    "        if self.time_step < self.num_arms:\n",
    "            return self.time_step\n",
    "        ucb_values = self.arm_rewards / self.arm_counts + np.sqrt(2 * np.log(self.time_step) / self.arm_counts)\n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "        self.time_step += 1\n",
    "\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, num_arms, epsilon):\n",
    "        self.num_arms = num_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.arm_counts = np.zeros(num_arms)\n",
    "        self.arm_rewards = np.zeros(num_arms)\n",
    "        self.time_step = 0\n",
    "    \n",
    "    def select_arm(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.num_arms)\n",
    "        return np.argmax(self.arm_rewards / self.arm_counts)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        self.arm_counts[arm] += 1\n",
    "        self.arm_rewards[arm] += reward\n",
    "        self.time_step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6a3b4-fceb-4ca9-9582-5696d9e9b18e",
   "metadata": {},
   "source": [
    "#### %%%%%%% YOU WILL EDIT THIS CELL AT SOME POINT %%%%%%% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66759ace-bac9-4e27-9944-590da18af63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianThompsonSampling:\n",
    "    def __init__(self, num_arms, prior_means, prior_variances, arm_noise_variances):\n",
    "        self.num_arms = num_arms\n",
    "        self.prior_means = prior_means\n",
    "        self.prior_variances = prior_variances\n",
    "        self.arm_noise_variances = arm_noise_variances\n",
    "    \n",
    "    def select_arm(self):\n",
    "        arm_samples = np.random.normal(self.prior_means, np.sqrt(self.prior_variances))\n",
    "        return np.argmax(arm_samples)\n",
    "\n",
    "    def update(self, arm, reward):\n",
    "        posterior_precision = 1.0 / self.prior_variances[arm] + 1.0 / self.arm_noise_variances[arm]\n",
    "        posterior_mean = (self.prior_means[arm] / self.prior_variances[arm] + reward / self.arm_noise_variances[arm]) / posterior_precision\n",
    "        self.prior_means[arm] = posterior_mean\n",
    "        self.prior_variances[arm] = 1.0 / posterior_precision\n",
    "\n",
    "class BayesianThompsonSamplingV2:\n",
    "    def __init__(self, num_arms, prior_means, prior_variances, arm_noise_variances):\n",
    "        self.num_arms = num_arms\n",
    "        self.prior_means = prior_means\n",
    "        self.prior_variances = prior_variances\n",
    "        self.arm_noise_variances = arm_noise_variances\n",
    "\n",
    "    def select_arm(self):\n",
    "        mean_diff = self.prior_means[1] - self.prior_means[0]\n",
    "        var_diff = self.prior_variances[1] + self.prior_variances[0]\n",
    "        p0 = norm.cdf(x=0, loc=mean_diff, scale=np.sqrt(var_diff)) # probability that P(mu1-mu0<0)\n",
    "        return 1-p0, np.random.choice(self.num_arms, p=[p0, 1-p0])\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        posterior_precision = 1.0 / self.prior_variances[arm] + 1.0 / self.arm_noise_variances[arm]\n",
    "        posterior_mean = (self.prior_means[arm] / self.prior_variances[arm] + reward / self.arm_noise_variances[arm]) / posterior_precision\n",
    "        self.prior_means[arm] = posterior_mean\n",
    "        self.prior_variances[arm] = 1.0 / posterior_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff66b0-d6b7-4cd5-8861-a7242a11fcb1",
   "metadata": {},
   "source": [
    "## Algorithm Comparison\n",
    "\n",
    "Now, let's compare the performance of different algorithms in the bandit environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa76717-006a-4d7c-8706-a0a5e8e0e756",
   "metadata": {},
   "source": [
    "#### %%%%%%% YOU WILL EDIT THIS CELL AT SOME POINT %%%%%%% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f213071-acba-4103-af72-c0e88124eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.random.seed(1000)\n",
    "np.random.seed(100)\n",
    "EVAL_BTS = True # CHANGE THIS TO True when you finish EXERCISE 1 / Editing BTS code.\n",
    "EVAL_BTSV2 = True # CHANGE THIS TO True when you finish EXERCISE 2 / Editing BTSV2 code.\n",
    "## %%%%%%% UNTIL HERE %%%%%%% \n",
    "\n",
    "NUM_ARMS  = 2\n",
    "TRUE_REWARDS = np.array([1., 0])\n",
    "NOISE_STD = 1.\n",
    "NOISE_STD_EST = 1.\n",
    "assert(len(TRUE_REWARDS)==NUM_ARMS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11cec2-4c8c-4965-9725-9202a5fce55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_arms = 2\n",
    "num_rounds = 1000\n",
    "prior_means = np.zeros(num_arms) \n",
    "prior_variances = np.ones(num_arms)\n",
    "arm_noise_variances = NOISE_STD_EST**2 * np.ones(num_arms)\n",
    "epsilon = 0.1\n",
    "\n",
    "# Create Bandit environment\n",
    "bandit_env = BanditEnvironment(num_arms=NUM_ARMS, true_rewards=TRUE_REWARDS)\n",
    "true_rewards = bandit_env.true_rewards\n",
    "optimal_reward = np.max(bandit_env.true_rewards)\n",
    "optimal_arm = np.argmax(bandit_env.true_rewards)\n",
    "\n",
    "# Create algorithms\n",
    "ucb = UCB(num_arms)\n",
    "eps_greedy = EpsilonGreedy(num_arms, epsilon)\n",
    "\n",
    "# Create a list of algorithms\n",
    "algs = [\n",
    "    ('UCB', ucb),\n",
    "    ('EpsilonGreedy', eps_greedy)\n",
    "]\n",
    "if EVAL_BTS:\n",
    "    bts = BayesianThompsonSampling(num_arms, prior_means, prior_variances, arm_noise_variances)\n",
    "    algs.append(('Bayes. Thomp. Sampling', bts))\n",
    "if EVAL_BTSV2:\n",
    "    btsv2 = BayesianThompsonSamplingV2(num_arms, prior_means, prior_variances, arm_noise_variances)\n",
    "    algs.append(('Bayes. Thomp. Sampling V2', btsv2))\n",
    "    p_bts2 = np.zeros(num_rounds) # probabilities of selecting arm\n",
    "\n",
    "# Create an empty dictionary to store results for each algorithm\n",
    "results = {alg: {\n",
    "    'chosen_arms': np.zeros(num_rounds, dtype=int),\n",
    "    'rewards': np.zeros(num_rounds),\n",
    "    'cumulative_rewards': np.zeros(num_rounds),\n",
    "    'cumulative_regret': np.zeros(num_rounds),\n",
    "    'cumulative_pulls': np.zeros(num_rounds)\n",
    "} for alg, _ in algs}\n",
    "\n",
    "for t in range(num_rounds):\n",
    "    for alg, alg_obj in algs:\n",
    "        # Select arm and pull it\n",
    "        if alg == \"Bayes. Thomp. Sampling V2\":\n",
    "            p_bts2[t], chosen_arm = alg_obj.select_arm()\n",
    "        else:\n",
    "            chosen_arm = alg_obj.select_arm()\n",
    "        reward = bandit_env.pull_arm(chosen_arm)\n",
    "        regret = optimal_reward-true_rewards[chosen_arm]\n",
    "\n",
    "        # Update algorithm\n",
    "        alg_obj.update(chosen_arm, reward)\n",
    "        \n",
    "        # Store results in the dictionary\n",
    "        results[alg]['chosen_arms'][t] = chosen_arm\n",
    "        results[alg]['rewards'][t] = reward\n",
    "        results[alg]['cumulative_rewards'][t] = results[alg]['cumulative_rewards'][t-1] + reward if t>0 else reward\n",
    "        results[alg]['cumulative_regret'][t] = results[alg]['cumulative_regret'][t-1] + regret if t>0 else regret\n",
    "        results[alg]['cumulative_pulls'][t] = (results[alg]['cumulative_pulls'][t-1]*t+(chosen_arm==optimal_arm))/(t+1) if t>0 else chosen_arm==optimal_arm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925f7ea-4f8c-4399-b92b-67c3387f45e2",
   "metadata": {},
   "source": [
    "# Plot cumulative rewards, cumulative regret, and number of pulls for each algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a1221e-c4c5-41c8-98fc-de9e3ab876e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for alg, data in results.items():\n",
    "    axs[0].plot(range(num_rounds), data['cumulative_rewards'], label=alg)\n",
    "    axs[1].plot(range(num_rounds), data['cumulative_regret'], label=alg)\n",
    "    axs[2].plot(range(num_rounds), data['cumulative_pulls'], label=alg)\n",
    "\n",
    "axs[0].set_ylabel(\"Cumulative Rewards\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_ylabel(\"Cumulative Regret\")\n",
    "\n",
    "axs[2].set_ylabel(\"Cumulative Number of Optimal Arm Pulls/Time Step\")\n",
    "\n",
    "for j in range(3):\n",
    "    axs[j].set_xlabel(\"Time Step\")\n",
    "    pplot(axs[j])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e33990-05f5-4b5b-926a-549cbce8b91b",
   "metadata": {},
   "source": [
    "\n",
    "## Questions\n",
    "\n",
    "1. What would happen to the regrets of different algorithms if we:\n",
    "   - Reduce the separation between the two arms?\n",
    "   - Increase noise variance in rewards?\n",
    "   - Noise variance estimate was wrong?\n",
    "   - Increase the number of rounds?\n",
    "2. BONUS: Try to empirically check the scaling rate of regret with the number of decision times using a Log-scale and least squares fit.\n",
    "3. Play around with random seeds and other problem parameters. Try something fun and creative. If you find something interesting, share it with us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b0a56-92a1-4976-971a-34c2d0139607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
